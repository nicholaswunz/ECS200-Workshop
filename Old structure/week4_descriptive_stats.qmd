# Descriptive Stats

**Week 4** - Learning descriptive stats and plotting data

In this workshop, you will learn how to summarise different types of ecological data using appropriate statistical measures and visualisations. Understanding the right measures of central tendency and spread is critical before conducting any formal hypothesis testing.


![](images/base_visual.png)

## Background reading {.unnumbered}


Before we can decide on the appropriate descriptive stats it helps to understand the **scale** of your variable.
The table below outlines appropriate measures for both qualitative and quantitative variables:

| Data Scale | Type         | Definition                                      | Examples                          | Central Tendency | Spread/Variation              |
|:----------:|:------------:|:-----------------------------------------------:|:---------------------------------:|:----------------:|:-----------------------------:|
| Nominal    | Qualitative  | Named categories with no intrinsic order        | sex (male/female), species name   | Mode             | Count, Proportion             |
| Ordinal    | Qualitative  | Named categories data with a natural order      | life stage (egg, juvenile, adult) | Median, Mode     | Range, IQR                    |
| Interval   | Quantitative | Equal intervals between values but no true zero | temperature, pH, dates            | Mean, Median     | SD, SE, CI                    |
| Ratio      | Quantitative | Equal intervals and a true zero                 | length, age, body mass            | Mean, Median     | SD, SE, CV, IQR, CI, variance | 


Key Terms:

- **Mean**: Average value. Use with symmetric, normally distributed data.
- **Median**: Middle value. More robust to outliers and skewed data.
- **Mode**: Most frequent value (useful for categories).
- **Standard Deviation (SD)**: How spread out the data is.
- **Standard Error (SE)**: SD divided by the square root of n (how precise the mean is).
- **Coefficient of Variation (CV)**: SD / Mean. Compares spread across variables.
- **Interquartile Range (IQR)**: Range between 25th and 75th percentiles.
- **Confidence Interval (CI)**: A range that likely contains the true mean.
- **Variance**: How much values differ from the average value.


If you are still confused, the CrashCourse Statistics videos on the [Mean, Median, and Mode: Measures of Central Tendency](https://youtu.be/kn83BA7cRNM?si=qzoPZk44hZRlzo9P) (11:22 min long), and the [Measures of Spread](https://youtu.be/R4yfNi_8Kqw?si=9QO_QzRmLG5oyQjH) (11:46 min long) better explains the terms visually.


Deciding on whether to use mean, median, or mode will depend on how distributed your data is. Here are some examples of the most common types of distributions encounters and how they affect the mean, median, and mode.



Normal distribution ("bell-curve") vs skewed distribution (positive and negative skews), bimodal distribution 
Create distrbution plots

Here is a CrashCourse Statistics video on [The Shape of Data: Distributions](https://youtu.be/bPFNxD3Yg6U?si=MSoRYOukKdRFjGem) (11:22 min long)






# Data types  {.unnumbered}

Before we start, load your previous homework workflow with the `plant_homework_data_clean.csv` dataset (errors removed). Here, we will call the imported as `plant_homework_clean_data`.

```{r load data, include=FALSE}
#install.packages("visdat")

library(tidyverse)
library(visdat)

# Load a CSV file
plant_homework_clean_data <- read_csv("C:/Users/75002992/OneDrive - Murdoch University/Teaching/ECS200/ECS200 - Workshop/data/plant_calcium_homework.csv") %>%
  dplyr::mutate(watershed = recode(watershed, "W2" = "W1"),
                watershed = as.factor(watershed),
                transect  = as.factor(transect)) %>%
  dplyr::filter(stem_dry_mass < 1)

```


::: {.panel-tabset .nav-pills}

## Filter rows with filter() {.unnumbered}

![](images/transform_filter.png)

`filter()` allows you to subset observations based on their values.
The first argument is the name of the data frame.
The second and subsequent arguments are the expressions that filter the data frame.
For example, we can select all values for the year 2004 for the calcium-treated sites (W1):


```{r filter, eval=FALSE}

# filter function by two columns. A numeric and a categorical.
filter(plant_homework_clean_data, year == 2004, watershed == "W1")

```

You can also filter rows by more one variable in a column using `x %in% y` logic.  This will select every row where `x` is one of the values in `y`.
For example, we can select all values in transect R1 and W1-1 only:

```{r filter2, eval=FALSE}

# filter function for the transect column by "R1" and "W1-1"
filter(plant_homework_clean_data, transect %in% c("R1", "W1-1"))

```

Lastly, you can filter by number thresholds. e.g. filtering out extreme values (i.e. for data quality checks), or filter within a range of values.
For example, we can select all `stem_dry_mass values` below 0.9 or between 0.01-0.02:

```{r filter3, eval=FALSE}

# filter function by stem_dry_mass values below 0.9
filter(plant_homework_clean_data, stem_dry_mass <= 0.9)

# filter function by stem_dry_mass values between 0.01 and 0.02
filter(plant_homework_clean_data, stem_dry_mass > 0.01 & stem_dry_mass < 0.02)

```

Other possible functions and operators within the `filter()` function can be found [here](https://dplyr.tidyverse.org/reference/filter.html).


## Arrange rows with arrange() {.unnumbered}

![](images/transform_arrange.png)

`arrange()` works similarly to `filter()` except that instead of selecting rows, it changes their order.
It takes a data frame and a set of column names (or more complicated expressions) to order by.
If you provide more than one column name, each additional column will be used to break ties in the values of preceding columns:


```{r arrange, eval=FALSE}

# Order the dataset by the ascending stem_length value (lowest in top row to highest in bottom row)
arrange(plant_homework_clean_data, stem_length)

# Order the dataset by the descending stem_length value (highest in top row to lowest in bottom row)
arrange(plant_homework_clean_data, desc(stem_length))

```

**Note**: Missing values are always sorted at the end.


## Select columns with select() {.unnumbered}

![](images/transform_select.png)

It‚Äôs not uncommon to get datasets with hundreds or even thousands of variables.
In this case, the first challenge is often narrowing in on the variables you‚Äôre actually interested in.
`select()` allows you to rapidly zoom in on a useful subset using operations based on the names of the variables.


```{r select, eval=FALSE}

# Select only three varaibles to keep.
select(plant_homework_clean_data, year, watershed, stem_dry_mass)

```


## Add new variables with mutate() {.unnumbered}

![](images/transform_mutate.png)

Besides selecting sets of existing columns, it‚Äôs often useful to add new columns that are functions of existing columns. That‚Äôs the job of `mutate()`.

`mutate()` always adds new columns at the end of your dataset so we‚Äôll start by creating a narrower dataset so we can see the new variables. 

```{r mutate, eval=FALSE}

# Calculate the total dry mass from leaf_dry_mass and stem_dry_mass and correct the unit from kilogram to gram
plant_homework_clean_data <- mutate(plant_homework_clean_data,
                                   total_dry_mass = leaf_dry_mass + stem_dry_mass,
                                   total_dry_mass_g = total_dry_mass * 1000) # convert kilogram to gram

```


## Grouped summaries with summarise() {.unnumbered}

![](images/transform_summarise.png)

The last key verb is `summarise()`. It collapses a data frame to a single row. 
However, `summarise()` is not terribly useful unless we pair it with `group_by()`. 
This changes the unit of analysis from the complete dataset to individual groups.
Then, when you use the dplyr verbs on a grouped data frame they‚Äôll be automatically applied ‚Äúby group‚Äù. 
For example, if we applied exactly the same code to a data frame (`plant_homework_clean_data`) grouped by watershed, we get the average values per treatment:

```{r summarise, eval=FALSE}

# Get the mean total_dry_mass_g by treatment groups
plant_homework_clean_data %>%
  group_by(watershed) %>%
  summarise(mean = mean(total_dry_mass_g))

```

What happens if we try to summarise a dataset with NA's inside?

```{r NA data, eval=FALSE}

# Here is an example dataset
NA_data <- data.frame(site = c("A", "B", "A", "B", "A", "B"),
                      species = c(3, 4, 6, 2, 4, NA))


# Get the mean species by site
NA_data %>%
  group_by(site) %>%
  summarise(mean = mean(species))


# Try this now with na.rm = TRUE
NA_data %>%
  group_by(site) %>%
  summarise(mean = mean(species, na.rm = TRUE))

```


That‚Äôs because aggregation functions obey the usual rule of missing values: if there‚Äôs any missing value in the input, the output will be a missing value.
Fortunately, all aggregation functions have an na.rm argument which removes the missing values prior to computation. If your dataset has NAs, the `na.rm = TRUE` argument will be helpful.

:::

## Exercise (10 min) {.unnumbered}

üß™ **Use the previous functions to complete the following task for the `fake_data` below**

```{r ex 1}

fake_data <- data.frame(
  site = c("A", "B", "C", "A", "B", "A", "B", "C", "A", "B", "A", "B", "A", "B", "C", "C", "B", "A", "b", "C", "B", "C", "C", "A", "A", "A", "A", "B", "C", "C", "C"),
  species_1 = as.integer(c(7, 25, 30, 50, 38, 13, 5, 31, 34, 22, 41, 25, 22, 37, 39, 13, 45, 10, 40, 7, 28, 36, 19, 2, 12, 5, 16, 38, 27, 44, 44)),
  species_2 = as.integer(c(9, 8, 1 , 7, 9 , 9, 4, 8, 1, 7, 1, 6, 4, 4, 7, 2, 8, 7, 2, 9, 9, 2, 6, 8, 6, 6, 17, 1, 8, 4, 4)),
  species_3 = as.integer(c(190, 160, 70, 430, 310, 99, 530, 420, 371, 357, 198, 171, 463, 124, 254, 484, 435, 409, 122, 305, 410, 162, 473, 200, 401, 273, 421, 419, 293, 487, 487)),
  species_4 = as.integer(c(2, 0, 4, 8, 6, 3, 7, 4, 2, NA, 35, 10, 6, 4, 8, 1, 1, 0, NA, 4, 6, 8, 10, 6, 2, 5, 10, 7, NA, NA, NA))
  )

```


- Conduct the usual **quality check** on this dataset (slightly different mistakes).
To save time, there is an inconsistent coding in one of the categorical variables, and an extreme value in one of the numeric variables to remove.
- Create a new variable column called `total_sp` that combines all the species.
- Select only the `site` and `total_sp` variable into a new dataframe.
- Calculate the mean, minimum, maximum, and count of `total_sp` by site.


Question:

1. What is the mean total abundance in site B?
2. WWhat is the problem with using mean here? Is there a better way to calculate central tendancy for dount data? (**hint**: think of decimals in this context).
3. Which site has the highest average species abundance?
4. Which site has the most replicate samples?


```{r ans 1, include=FALSE, eval=FALSE}

str(fake_data)

fake_data_clean <- fake_data %>%
  dplyr::mutate(site = recode(site, "b" = "B"),
                site = as.factor(site)) %>%
  dplyr::filter(species_4 < 30) %>%
  dplyr::mutate(total_sp = species_1 + species_2 + species_3 + species_4) %>%
  dplyr::select(site, total_sp)

fake_data_clean %>%
  dplyr::group_by(site) %>%
  dplyr::summarise(mean = mean(total_sp),
                   min  = min(total_sp),
                   max  = max(total_sp),
                   n = n())

# 1. 361.25 
# 2. Mode more suitable for integers
# 3. Site B
# 4. site A

```


# Tidy code {.unnumbered}

Good coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread.
I mostly comfortable with the tidyeverse syntax, but the most important take away here is **keep everything consistent** if you prefer a different style.


The full breakdown can be found [here](https://style.tidyverse.org/).

::: {.panel-tabset .nav-pills}

## File names {.unnumbered}

File names should be **machine readable**: avoid spaces, symbols, and special characters.
Prefer file names that are all lower case, and never have names that differ only in their capitalisation. 
Delimit words with `-` or `_`. Use .R as the extension of R files.

```{r file 1, eval=FALSE}

# Good
fit_models.R
utility_functions.R
exploratory-data-analysis.R

# Bad
fit models.R
foo.r
ExploratoryDataAnaylsis.r

```

File names should be **human readable**: use file names to describe what‚Äôs in the file.

```{r file 2, eval=FALSE}

# Good
report-draft-notes.txt

# Bad
temp.r
```

Use the same structure for closely related files:

```{r file 3, eval=FALSE}
# Good
fig-eda.png
fig-model-3.png

# Bad
figure eda.PNG
fig model three.png

```


## R organisation {.unnumbered}

Use commented lines of `-` to break up your file into easily readable chunks.
Here is a **template** you can use for your future projects. 

```{r organisation}
# Title.
# Short description of what is being done.
# Authors name and date of analysis.


## Set up ## ----------------------------------------------------------------

# any code related to setting up directory, installing and loading packages (e.g. from workshop 1)

## Loading and cleaning data ## ---------------------------------------------

# any code relating to loading the data, checking the quality, transforming the dataset, so that it is ready for analysis


## Analysis ## -------------------------------------------------------------

# All code involved with preliminary analysis, full analysis, and model output

## Visualisation ## --------------------------------------------------------

# Creating beautiful figures for your report!

```


## Syntax {.unnumbered}

### Object names {.unnumbered}

Variable and function names should use only lowercase letters, numbers, and _. Use underscores (_) (so called snake case) to separate words within a name.

```{r syntax 1, eval=FALSE}

# Good
day_one
day_1

# Bad
DayOne
dayone

```

Generally, variable names should be nouns and function names should be verbs. Strive for names that are concise and meaningful (this is not easy!).

```{r syntax 2, eval=FALSE}

# Good
day_one

# Bad
first_day_of_the_month
djm1

```

Where possible, avoid re-using names of common functions and variables. This will cause confusion for the readers of your code.

```{r syntax 3, eval=FALSE}

# Bad
T <- FALSE
c <- 10
mean <- function(x) sum(x)

```

### Infix operators {.unnumbered}

Most infix operators (`==`, `+`, `-`, `<-`, etc.) should always be surrounded by spaces:

```{r infix 1, eval=FALSE}

# Good
height <- (feet * 12) + inches
mean(x, na.rm = TRUE)

# Bad
height<-feet*12+inches
mean(x, na.rm=TRUE)

```

There are a few exceptions, which should never be surrounded by spaces:

The operators with [high precedence](https://rdrr.io/r/base/Syntax.html): `::`, `:::`, `$`, `@`, `[`, `[[`, `^`, unary `-`, unary `+`, and `:`.

```{r infix 2, eval=FALSE}

# Good
sqrt(x^2 + y^2)
df$z
x <- 1:10

# Bad
sqrt(x ^ 2 + y ^ 2)
df $ z
x <- 1 : 10

```

### Assignment {.unnumbered}

Use `<-`, not `=`, for assignment.

```{r assignment, eval=FALSE}

# Good
x <- 5

# Bad
x = 5

```

## Pipes {.unnumbered}

Use `%>%` or `|>` to emphasise a sequence of actions, rather than the object that the actions are being performed on.

The tidyverse has been designed to work particularly well with the pipe, but you can use it with any code, particularly in conjunction with the _ placeholder.

```{r pipes, eval=FALSE}

strings %>%
  str_replace("a", "b") %>%
  str_replace("x", "y")

strings |>
  gsub("a", "b", x = _) |>
  gsub("x", "y", x = _)

```

Avoid using the pipe when:

- You need to manipulate more than one object at a time. Reserve pipes for a sequence of steps applied to one primary object.
- There are meaningful intermediate objects that could be given informative names.

### Long lines {.unnumbered}

If the arguments to a function don‚Äôt all fit on one line, put each argument on its own line and indent:

```{r long, eval=FALSE}

# Good
iris %>%
  summarise(
    Sepal.Length = mean(Sepal.Length),
    Sepal.Width = mean(Sepal.Width),
    .by = Species
  )

iris |>
  summarise(
    Sepal.Length = mean(Sepal.Length),
    Sepal.Width = mean(Sepal.Width),
    .by = Species
  )

# Bad
iris %>%
  summarise(Sepal.Length = mean(Sepal.Length), Sepal.Width = mean(Sepal.Width), .by = Species)

```


For data analysis, we recommend using the pipe whenever a function needs to span multiple lines, even if it‚Äôs only a single step.

```{r long 2, eval=FALSE}

# Bad
summarise(
  iris,
  Sepal.Length = mean(Sepal.Length),
  Sepal.Width = mean(Sepal.Width),
  .by = Species
)

```


## Conflict packages {.unnumbered}

Take careful note of the conflicts message that‚Äôs printed when you load the tidyverse. It tells you that dplyr overwrites some functions in base R.
If you want to use the base version of these functions after loading dplyr, you‚Äôll need to use their full names: `stats::filter()` and `dplyr::filter()`.


:::

## Exercise (15 min) {.unnumbered}

üß™ **Clean the code below**

Your colleague got a new job and you have been promoted to his position where you are taking over his unfinished projects. 
The ecological data has been collected and some quick analysis was performed.
But, you noticed that he didnt perform a quality check on the raw data and the # explanations are not clear.
The current state is not suitable for the client so you are assigned to clean.

Clean the code where relevent, and check if all the code is working on your end.

**Note**: change the working directory to your folder and load the plant_calcium.csv to work on.
**Note 2**: The same errors to fix are from the [week 2 - data quality workshop](week2_data_exploration.qmd).


```{r ex 2, eval=FALSE}

# Exercise 2
# Cleaning code and tidying dataset
# John Smith, 25-03/2025

## Set up ## ----------------------------------------------------------------
library(tiderVerse)

# Set my directory
setwd("your working directory")

## Loading and cleaning data ## ---------------------------------------------

# Load raw data
plant data <- read_csv("data/plant_calcium.csv")
str(plant data)

## Analysis ## -------------------------------------------------------------

# Fit model: height explained by elevation
model <- lm(stem_dry_mass ~ stem_length, data = plant_data)

# Check model summary
summary(model)

## Visualisation ## --------------------------------------------------------

# Creating scatterplot figure
plant_data %>%
  ggplot() +
  geom_point(aes(x = stem_length, y = Stem_dry_mass)) +
  labs(
    x = "Stem Length (mm)",
    y = "Stem Dry Mass (g)",
    title = "Stem Dry Mass vs. Stem Length in Sugar Maple Seedlings",
    subtitle = "Hubbard Brook LTER"
  ) +
  theme_minimal()

```


```{r ans 2, include=FALSE, eval=FALSE}

# Exercise 2
# Cleaning code and tidying dataset
# John Smith, 25-03-2025

## Set up ## ----------------------------------------------------------------
library(tidyverse)

# Set my directory
setwd("C:/Users/75002992/OneDrive - Murdoch University/Teaching/ECS200/ECS200 - Workshop")

## Loading and cleaning data ## ---------------------------------------------

# Load raw data
plant_data <- read_csv("data/plant_calcium.csv")
str(plant_data)


# Correct class type
plant_data <- plant_data %>%
  mutate(watershed = as.factor(watershed),
         elevation = as.factor(elevation))

# Check for missing values in each column
colSums(is.na(plant_data))

# Visualise missing data pattern
visdat::vis_dat(plant_data_clean)

# Remove NA rows based on missing elevation data
plant_data <- plant_data %>% 
  drop_na(elevation)

# Check for duplicate rows
sum(duplicated(plant_data))  # None

# Check for outliers

ggplot(plant_data, aes(x = leaf2area)) +
  geom_histogram() +
  theme_bw()

# create a clean dataset after filtering the outlier
plant_data_clean <- plant_data %>%
  dplyr::filter(leaf2area < 90)

## Analysis ## -------------------------------------------------------------

# Fit model: height explained by elevation
dry_length_mod <- lm(stem_dry_mass ~ stem_length, data = plant_data_clean)

# Check model summary
summary(dry_length_mod)

## Visualisation ## --------------------------------------------------------

# Creating scatterplot figure
plant_data_clean %>%
  ggplot() +
  geom_point(aes(x = stem_length, y = stem_dry_mass)) +
  labs(
    x = "Stem Length (mm)",
    y = "Stem Dry Mass (g)",
    title = "Stem Dry Mass vs. Stem Length in Sugar Maple Seedlings",
    subtitle = "Hubbard Brook LTER"
  ) +
  theme_minimal()

```

# Homework {.unnumbered}

Fortunately, the authors of the study cleaned the data so we do not need to do much data transformation before analysis.
For the purpose of next weeks workshop, please filter your `plant_homework_data_clean` data by selecting only `year`, `watershed`, `elevation`, `stem_length`, `leaf_dry_mass`, and `stem_dry_mass`.

Next, calculate the total dry mass from `leaf_dry_mass` and `stem_dry_mass` and correct the unit from kilogram to gram.

Create a separate object to that summarises the mean, standard deviation, and count `plant_homework_data_clean` of the total dry mass variable by `watershed` and `year`.

Save this workflow for next weeks workshop.
Remember to create a header with the next workshop titled ‚ÄúWeek 4 workshop - Descriptive statistics‚Äù, a description called ‚ÄúRun some basis statistical summarise and plotting the data‚Äù, your name, and the next workshops date.

```{r home 1, include=FALSE, eval=FALSE}

plant_data_clean <- plant_homework_clean_data %>%
  dplyr::select(year, watershed, elevation, stem_length, leaf_dry_mass, stem_dry_mass) %>%
  dplyr::mutate(total_mass_g = (leaf_dry_mass + stem_dry_mass) * 1000)


plant_data_clean %>%
  dplyr::group_by(watershed, year) %>%
  dplyr::summarise(mean = mean(total_mass_g),
                   sd   = sd(total_mass_g),
                   n    = n())

```


## For the adventurous folks {.unnumbered}

You've learned how to use the core `dplyr` verbs ‚Äî now let‚Äôs practice building more powerful and efficient workflows by combining multiple functions in clean, compact code.
Here are your advanced challenges for next week:

**1. Take your cleaned dataset `plant_homework_data_clean` and write one pipeline that**:

- Filters only `W1` watershed and year `2004`.
- Creates a new variable `total_mass_g` from `stem_dry_mass` + `leaf_dry_mass`, multiplied by 1000
- Selects only the new variable and `stem_length`
- Filters any rows with `stem_length` greater than 200 mm.
- Arranges the result by `total_mass_g` in descending order.

Use `%>%` or `|>` and avoid creating intermediate objects.

```{r hard 1, include=FALSE, eval=FALSE}

plant_homework_clean_data %>%
  dplyr::filter(year == 2004 , watershed == "W1") %>%
  dplyr::mutate(total_mass_g = (leaf_dry_mass + stem_dry_mass) * 1000) %>%
  dplyr::select(total_mass_g, stem_length) %>%
  dplyr::arrange(desc(total_mass_g))

```

**2. Using `group_by()` and `summarise()` in a single chain**:

- Group the data by `watershed` and `year`.
- Calculate the mean, coefficient of variation, and count of `total_mass_g`.
- Arrange by year in ascending order.

```{r home 2, include=FALSE, eval=FALSE}

plant_homework_clean_data %>%
  dplyr::group_by(watershed, year) %>%
  dplyr::summarise(mean = mean(total_dry_mass_g),
                   cv   = sd(total_dry_mass_g) / mean * 100, 
                   n    = n())

```


**3. Write a function that**:

- Takes a numeric column as input.
- Filters out outliers using the 1.5 √ó IQR rule.
- Returns the filtered data.

Try applying it to `leaf_dry_mass` and `stem_length`.

```{r home 3, include=FALSE, eval=FALSE}

remove_outliers <- function(x) {
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  x > (Q1 - 1.5 * IQR) & x < (Q3 + 1.5 * IQR)
}

# This returns a logical vector (TRUE = keep, FALSE = outlier).


# Keep rows where both variables are within IQR bounds (i.e. not outliers)
plant_no_outliers <- plant_homework_clean_data %>%
  dplyr::filter(remove_outliers(leaf_dry_mass),
                remove_outliers(stem_length)
                )


nrow(plant_homework_clean_data)  # Original 358
nrow(plant_no_outliers)          # After filtering 345

```


# Extra Stuff {.unnumbered}

-




