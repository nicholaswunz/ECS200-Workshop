[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ECS200 - Research Methods in Ecology",
    "section": "",
    "text": "Welcome\nWorkshop materials in the github repository ECS200.\nThe ECS200 - Research Methods in Ecology workshops will go through the steps of a typical data science project that looks something like Figure 1.\nFirst, you must import (week 1) your data into R. This usually means loading data stored in a file, database, or web application programming interface (API) into a data frame in R. If you can‚Äôt get your data into R, you can‚Äôt do any data science with it!\nOnce your data is imported, it‚Äôs a good idea to tidy (week 2) it. Tidying means putting it into a consistent format where each column is a variable and each row is an observation. This alignment between the structure and meaning of the data makes analysis far easier. Tidy data lets you focus on answering questions rather than reformatting for each new task.\nNext, you‚Äôll often transform (week 2) your data. This might include filtering for specific observations (e.g.¬†animals from one site or data from the past year), creating new variables (e.g.¬†calculating speed from distance and time), or summarising patterns (e.g.¬†counts or averages). Tidying and transforming are often called wrangling, because getting data into a usable form can feel like a struggle!\nOnce you have tidy, relevant data, you‚Äôll typically turn to two main tools: visualisation and modelling. These are complementary, and most analyses move back and forth between them.\nThe final step in data science is communication (visualisation and model weeks). Even the best models or visuals are useless unless you can clearly explain your findings to others. Effective communication is critical.\nSurrounding all these activities is programming. You‚Äôll use code throughout every step of a data science project. You don‚Äôt need to be a programming expert, but improving your coding skills pays off‚Äîit helps you automate tasks and solve problems more efficiently.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1¬† Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. ‚ÄúLiterate Programming.‚Äù Comput. J. 27 (2): 97‚Äì111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2¬† Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Farkas, K. R., Brown, Z. A., King, J. L., Nicotra, A. B., Head,\nM. L. and Arnold, P. A. (2025). Combined heat and drought\naffect the abundance, composition and diversity of subalpine\nsurface-active soil arthropod communities. Ecological\nEntomology.\n\n\nJuice, S. M., Fahey, T. J., Siccama, T. G., Driscoll, C. T.,\nDenny, E. G., Eagar, C., Cleavitt, N. L., Minocha, R. and Richardson, A.\nD. (2006). Response\nof sugar maple to calcium addition to northern hardwood forest.\nEcology 87, 1267‚Äì1280.\n\n\nPeters, S. C., Blum, J. D., Driscoll, C. T. and Likens, G.\nE. (2004). Dissolution\nof wollastonite during the experimental manipulation of hubbard brook\nwatershed 1. Biogeochemistry 67, 309‚Äì329.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "index.html#learning-objectives-for-tutorial",
    "href": "index.html#learning-objectives-for-tutorial",
    "title": "ECS200 - Applied Ecological Approaches",
    "section": "Learning Objectives for Tutorial",
    "text": "Learning Objectives for Tutorial\nAt the end of the tutorial, participants will be able to:\n\nIdentify the main features that make Julia an attractive language for Data Science\nSet up a Julia environment to run their data analysis\nEfficiently handle datasets (even across different languages) through Tables.jl and Arrow.jl\nFit (generalized) linear mixed models with MixedModels.jl\nCommunicate across languages (Julia, R, python)\n\nIntended audience and level: The tutorial is intended for any data scientist with experience in R and/or python who is interested in learning the attractive features of Julia for Data Science. No knowledge of Julia is required.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#learning-objectives-of-the-unit",
    "href": "index.html#learning-objectives-of-the-unit",
    "title": "ECS200 - Research Methods in Ecology",
    "section": "Learning Objectives of the Unit",
    "text": "Learning Objectives of the Unit\nAt the end of the unit, students should be able to:\n\nDesign ecological research using appropriate sampling strategies and analytical methods.\nApply appropriate statistical methods to test ecological hypotheses.\nApply R programming to process, analyse, and visualise ecological datasets to professional standards.\nCritically evaluate the strengths and limitations of research designs under different ecological conditions.\nCommunicate the findings of ecological research to professional standards.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "ECS200 - Applied Ecological Approaches",
    "section": "Schedule",
    "text": "Schedule\n\n\n\nWeek\nTopic\nDate\nTime\nRoom\n\n\n\n\n1\nIntroduction to R and RStudio\n23-Feb\n9:00 - 10:00\nTBA\n\n\n2\nData quality checks and data exploration\n02-Mar\n9:00 - 10:00\nTBA\n\n\n3\nTidy data for analysis, clean code\n09-Mar\n9:00 - 10:00\nTBA\n\n\n4\nDescriptive statistics & data exploration\n16-Mar\n9:00 - 10:00\nTBA\n\n\n5\nCalculate diversity indices\n23-Mar\n9:00 - 10:00\nTBA\n\n\n6\nStatistical analysis\n30-Mar\n9:00 - 10:00\nTBA\n\n\n7\nStudy Week\n\n\n\n\n\n8\nPresenting statistical results\n13-Apr\n9:00 - 10:00\nTBA\n\n\n9\nVisualise results with ggplot2\n20-Apr\n9:00 - 10:00\nTBA\n\n\n10\nStudy Week\n\n\n\n\n\n11\nCommunicating and writing reports\n04-May\n9:00 - 10:00\nTBA\n\n\n12\nHow to write good reports\n11-May\n9:00 - 10:00\nTBA\n\n\n13\nHow to reference properly\n18-May\n9:00 - 10:00\nTBA\n\n\n14\nDiscussing limitations\n25-May\n9:00 - 10:00\nTBA",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "week1_getting_started.html",
    "href": "week1_getting_started.html",
    "title": "1¬† Quality Check",
    "section": "",
    "text": "Before we start\nWeek 1 - Quality checks and data exploration\nIn this first workshop, we‚Äôll introduce you to R, RStudio, and the basics of coding. You‚Äôll be writing your first lines of R code and importing data today!\nThis step can be skipped if you have R and Rstudio already downloaded and installed.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Quality Check</span>"
    ]
  },
  {
    "objectID": "week2_data_exploration.html",
    "href": "week2_data_exploration.html",
    "title": "2¬† Quality Check",
    "section": "",
    "text": "Background reading\nWeek 2 - Quality checks and data exploration\nIn this workshop, you will learn how to check the quality of the raw data you imported, and steps to correct errors in the raw data.\nHow do we know the data is ready for analysis? Before we do any analysis, we need to check the raw data for the following:\nThese steps help ensure your data is reliable, interpretable, and suitable for statistical analysis. Importantly, it makes you a trustworthy researcher!",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Quality Check</span>"
    ]
  },
  {
    "objectID": "week1_getting_started.html#rstudio-layout",
    "href": "week1_getting_started.html#rstudio-layout",
    "title": "1¬† Getting Started with R",
    "section": "1.2 RStudio layout",
    "text": "1.2 RStudio layout\nGet familar with the layout.\n\n\n\nFig. 1. Layout with the four major windows (or panals) of the RStudio environment.\n\n\n\nSource: This is where you will write/view R scripts. Some outputs (such as if you view a dataset using View()) will appear as a tab here.\nConsole/Terminal/Jobs: This is where you see the execution of commands. You can work interactively (i.e.¬†enter R commands here), but for the most part we will run a script (or lines in a script) in the source pane and watch their execution and output here. The ‚ÄúTerminal‚Äù tab give you access to the BASH terminal (the Linux operating system, unrelated to R). RStudio also allows you to run jobs (analyses) in the background. This is useful if some analysis will take a while to run. You can see the status of those jobs in the background.\nEnvironment/History: Here, RStudio will show you what datasets and objects (variables) you have created and which are defined in memory. You can also see some properties of objects/datasets such as their type and dimensions. The ‚ÄúHistory‚Äù tab contains a history of the R commands you‚Äôve executed R.\nFiles/Plots/Packages/Help/Viewer: This multipurpose pane will show you the contents of directories on your computer. You can also use the ‚ÄúFiles‚Äù tab to navigate and set the working directory. The ‚ÄúPlots‚Äù tab will show the output of any plots generated. In ‚ÄúPackages‚Äù you will see what packages are actively loaded, or you can attach installed packages. ‚ÄúHelp‚Äù will display help files for R *functions and packages. ‚ÄúViewer‚Äù will allow you to view local web content (e.g.¬†HTML outputs).",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting Started with *R*</span>"
    ]
  },
  {
    "objectID": "week1_getting_started.html#loading-packages",
    "href": "week1_getting_started.html#loading-packages",
    "title": "1¬† Getting Started with R",
    "section": "1.3 Loading packages",
    "text": "1.3 Loading packages\nThe most common packages we will be using in this unit.\nThis is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting Started with *R*</span>"
    ]
  },
  {
    "objectID": "week1_getting_started.html#loading-files",
    "href": "week1_getting_started.html#loading-files",
    "title": "1¬† Getting Started with R",
    "section": "1.4 Loading files",
    "text": "1.4 Loading files\nThe most common file is .csv file.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting Started with *R*</span>"
    ]
  },
  {
    "objectID": "week1_getting_started.html#for-the-adventurous-folks",
    "href": "week1_getting_started.html#for-the-adventurous-folks",
    "title": "1¬† Quality Check",
    "section": "For the adventurous folks",
    "text": "For the adventurous folks\nFor those comfortable with R and coding and would like an additional challenge. Here is a task for you before the next week.\n\nCreate a vector tree_heights with 100 numeric values between 0-50 using the R random number generator.\nFind the quartile range of the from the tree_heightsvector.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Quality Check</span>"
    ]
  },
  {
    "objectID": "week1_getting_started.html#before-we-start",
    "href": "week1_getting_started.html#before-we-start",
    "title": "1¬† Getting Started with R",
    "section": "Before we start",
    "text": "Before we start\nThis step can be skipped if you have R and Rstudio already downloaded and installed.\n\nMurdoch computer\nAll Murdoch computers have R and Rstudio available. Login to your account and access Rstudio on your work computer.\n\n\nYour own computer\nIf you are using your own personal computer in this unit, here are the steps for downloading and installing R and Rstudio.\n\nInstall R\n\nGo to https://cran.r-project.org\nChoose your operating system and follow the instructions to install.\n\nInstall RStudio\n\nGo to https://posit.co/download/rstudio-desktop/\nDownload the free version of RStudio Desktop.\nInstall it on your computer.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "the_islands.html",
    "href": "the_islands.html",
    "title": "The Islands",
    "section": "",
    "text": "Design your project\nThe Island is a virtual environment where the original intention was for students to collect data for addressing questions in epidemiology. However, the Island has since been updated to include field stations that allow for plant growth. The plants can represent any plant, either an agriculturally significant plant, an indigenous significant plant, or a species of conservation concern. You decide on what plant it is and how it related to your topic.\nNote, the plants grow in real time, and typically take 14 days to fully grow, depending on the local climate.\nAccess The Island from myMurdoch Learning.\nThere are three islands of focus (Fig. 1):\nAltogether, there are three field stations from three islands with differing climates. Each station has 36 plots.\nYou can manipulate Nitrogen (N) and Phosphorus (P) levels in the soil prior to growing the plants. The level of N and P are represented by blacks. Two blocks are the default, so we would consider them normal levels (Fig. 2), no blocks mean no N or P, four blocks means twice the concentration of normal levels, and five blocks is the maximum allowed (represents 2.5 times normal levels). Give the blocks some values. Use realistic values based on the literature and what is considered normal for the plant of interest.\nBecause there are three stations that span across different climates. You can compare the effect of temperature or rainfall on plant growth, fruit production etc. All stations provide the date, maximum temperature in degrees Celsius and rainfall in millimetres (Table 1). You can use this data to correlate with your dependent variables. You have the option to look at the interaction of N or P with temperature or rainfall (factorial design).\nFor the 36 plots per site, it means you can have 18 replicate plots/treatment if you have two treatments, 12 replicate plots/treatment if you have three treatments, 9 replicate plots/treatment if you have four treatments, and 6 replicate plots/treatment if you have five treatments. Balance between number of treatments and number of replicates.",
    "crumbs": [
      "The Islands"
    ]
  },
  {
    "objectID": "the_islands.html#background",
    "href": "the_islands.html#background",
    "title": "The Islands",
    "section": "",
    "text": "Fig. 1. Layout of the islands with locations of the field and weather stations.\n\n\n\n\nIronbard: Ironbard is the most northern island and has a temperate climate (equivalent to Tasmania‚Äôs climate). The plant field station in Ironbard is called Hofn Field Station. The local weather station is called Abramsen Climate Station.\nProvidance: Providance is the island located on the east and has a semi-arid climate (equivalent to Perth‚Äôs climate). The plant field station in Providanceis called Biruwa Field Station. The local weather station is called Kennedy Climate Station.\nBonne Sante: Bonne Sante is the most southern island and has a subtropical climate (equivalent to Brisbane‚Äôs climate). The plant field station in Providanceis called Mutalau Field Station. The local weather station is called Nanu Forest Climate Station.",
    "crumbs": [
      "The Islands"
    ]
  },
  {
    "objectID": "the_islands.html#design-your-project",
    "href": "the_islands.html#design-your-project",
    "title": "The Islands",
    "section": "",
    "text": "Fig. 2. Example nitrogen (N) or phosphorus (P) block manipulation, where no blocks mean no N or P, two blocks is the default/normal N or P levels, four blocks means twice the concentration of normal levels, and five blocks is the maximum allowed (represents 2.5 times normal levels).\n\n\n\n\nTable 1: Example table output from the Kennedy Climate Station on Providence Island. Date is based on the simulation and presented by day, Max Temp is the maximum temperature (¬∞C) recorded on the day, and Rainfall is the cumulative rainfall of the day (mm).\n\n\nDate\nMax Temp (¬∞C)\nRainfall (mm)\n\n\n\n\n01/064\n18.8\n1.0\n\n\n02/064\n23.7\n0.8\n\n\n03/064\n25.3\n0.8\n\n\n04/064\n29.3\n0.0\n\n\n05/064\n29.7\n14.1\n\n\n\n\n\nWhat you can measure?\nYour plant will look like this (Fig. 3). The green parts represent leaves, the purple circles represent fruits, and the white circles represent flowers. You can measure the following as response dependent variables:\n\n\n\nFig. 3. Example plant after 10 days since germination.\n\n\nHeight: You can use a ruler to measure on your screen.\nGrowth rate: If you measure height over time, you can calculate growth rate. Note, make sure your screen is the same size to ensure consistency in your measurements over time.\nNumber of leaves: Count the number of green parts.\nNumber of fruits: Count the number of purple circles.\nNumber of flowers: Count the number of white spots.\nBranches: Count the number of diverging branches from the main stem.\nYield: Once harvested, you will get a number in the ‚ÄúHarvest‚Äù tab (Table 2). The data table will include Date of harvest, the row and column of the plot, and the yield or biomass (g). Once harvested, the plant doesn‚Äôt grow again (ONE time, destructive). So, be careful on when you plan to harvest the plant.\n\nTable 2: Example yield table output from Hofn Field Station on Ironbard. Date is based on the simulation and presented by day, Row and Column are the positon of the plant in the field grid, and Yield is total biomass value (g).\n\n\nDate\nRow\nColumn\nYield\n\n\n\n\n01/064\n1\n1\n320\n\n\n02/064\n1\n2\n269\n\n\n03/064\n1\n3\n327\n\n\n04/064\n1\n4\n390\n\n\n05/064\n1\n5\n307\n\n\n\nI have no idea what the red circles are. You can count them and assign a name to them if you like (e.g.¬†berries, fungal pathogen).",
    "crumbs": [
      "The Islands"
    ]
  },
  {
    "objectID": "week1_getting_started.html#why-r",
    "href": "week1_getting_started.html#why-r",
    "title": "1¬† Getting Started with R",
    "section": "",
    "text": "Open-source: Free to install and use ‚Äî forever.\nReproducible: Your code is your record of what you did.\nPowerful: Easily handles large datasets, statistics, modelling, and maps.\nCommunity: R has a huge global community of ecologists and scientists to help troubleshoot and develop better packages.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "week1_getting_started.html#step-2-open-rstudio",
    "href": "week1_getting_started.html#step-2-open-rstudio",
    "title": "1¬† Getting Started with R",
    "section": "1.1 Step 2: Open RStudio",
    "text": "1.1 Step 2: Open RStudio\nRStudio is your coding workspace ‚Äî it helps you write, run, and save your R code.\nRStudio has four panes:\n\nConsole: Where commands run.\nScript Editor: Where you write and save your code.\nEnvironment/History: See your variables and previous commands.\nPlots/Packages/Help/Files: View graphs and manage tools.\n\n\n\n\nFig. 1. Layout with the four major windows (or panals) of the RStudio environment.\n\n\n\nSource: This is where you will write/view R scripts. Some outputs (such as if you view a dataset using View()) will appear as a tab here.\nConsole/Terminal/Jobs: This is where you see the execution of commands. You can work interactively (i.e.¬†enter R commands here), but for the most part we will run a script (or lines in a script) in the source pane and watch their execution and output here. The ‚ÄúTerminal‚Äù tab give you access to the BASH terminal (the Linux operating system, unrelated to R). RStudio also allows you to run jobs (analyses) in the background. This is useful if some analysis will take a while to run. You can see the status of those jobs in the background.\nEnvironment/History: Here, RStudio will show you what datasets and objects (variables) you have created and which are defined in memory. You can also see some properties of objects/datasets such as their type and dimensions. The ‚ÄúHistory‚Äù tab contains a history of the R commands you‚Äôve executed R.\nFiles/Plots/Packages/Help/Viewer: This multipurpose pane will show you the contents of directories on your computer. You can also use the ‚ÄúFiles‚Äù tab to navigate and set the working directory. The ‚ÄúPlots‚Äù tab will show the output of any plots generated. In ‚ÄúPackages‚Äù you will see what packages are actively loaded, or you can attach installed packages. ‚ÄúHelp‚Äù will display help files for R *functions and packages. ‚ÄúViewer‚Äù will allow you to view local web content (e.g.¬†HTML outputs).",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "week1_getting_started.html#step-3-first-r-commands-ecological-examples",
    "href": "week1_getting_started.html#step-3-first-r-commands-ecological-examples",
    "title": "1¬† Getting Started with R",
    "section": "1.2 Step 3: First R Commands (Ecological Examples)",
    "text": "1.2 Step 3: First R Commands (Ecological Examples)\nClick File &gt; New File &gt; R Script to start.\nLet‚Äôs try typing and running some code:\n\n# Create a variable: number of frogs counted at a pond\nfrogs &lt;- 18\n\n# Add 5 frogs observed the next day\nfrogs_total &lt;- frogs + 5\n\n# Print result\nfrogs_total\n\n[1] 23\n\n\nTry this one too:\n\n# Mean leaf size of 5 Eucalyptus trees (cm¬≤)\nleaf_sizes &lt;- c(23.4, 21.8, 25.1, 19.7, 22.5)\n\n# Calculate the average leaf size\nmean(leaf_sizes)\n\nüß™ Challenge: How would you find the largest leaf size?\n\nmax(leaf_sizes)",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "week1_getting_started.html#loading-packages-and-files",
    "href": "week1_getting_started.html#loading-packages-and-files",
    "title": "1¬† Getting Started with R",
    "section": "1.3 Loading packages and files",
    "text": "1.3 Loading packages and files\nLet‚Äôs now load a data file!\nImagine we‚Äôve collected species abundance data from different habitats. The file is called species_counts.csv.\nFirst, save the file in your RStudio project folder (or Downloads folder for now).\nThe most common packages we will be using in this unit.\nThis is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n# Install packages using the install.packages() function.\ninstall.packages(\"tidyverse\")\n\n# Load the readr package (part of tidyverse)\nlibrary(tidyverse)\nlibrary(readr)\n\n# Load a CSV file\n#species_data &lt;- read_csv(\"species_counts.csv\")\n\n# View the first few rows\n#head(species_data)\n\nüìù Make sure your working directory is set to the folder where your data file is located. You can use getwd() to check and setwd(‚Äúpath/to/your/folder‚Äù) to change it.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "week1_getting_started.html#step-1-open-rstudio",
    "href": "week1_getting_started.html#step-1-open-rstudio",
    "title": "1¬† Getting Started with R",
    "section": "Step 1: Open RStudio",
    "text": "Step 1: Open RStudio\nRStudio has four panes:\n\nConsole: Where commands run.\nSource: Where you write and save your code.\nEnvironment/History: See your datasets and objects (variables) and previous commands (‚ÄúHistory‚Äù tab).\nPlots/Packages/Help/Files: View graphs and manage tools. You can also use the ‚ÄúFiles‚Äù tab to navigate and set the working directory. The ‚ÄúPlots‚Äù tab will show the output of any plots generated. In ‚ÄúPackages‚Äù you will see what packages are actively loaded, or you can attach installed packages. ‚ÄúHelp‚Äù will display help files for R functions and packages. ‚ÄúViewer‚Äù will allow you to view local web content (e.g.¬†HTML outputs).\n\n\n\n\nFig. 1. Layout with the four major windows (or panals) of the RStudio environment.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "week1_getting_started.html#step-2-first-r-commands-ecological-examples",
    "href": "week1_getting_started.html#step-2-first-r-commands-ecological-examples",
    "title": "1¬† Getting Started with R",
    "section": "Step 2: First R Commands (Ecological Examples)",
    "text": "Step 2: First R Commands (Ecological Examples)\nClick File &gt; New File &gt; R Script to start.\nStart by noting who is writing, the date, and the main goal - in our case, determining how many species from different taxa have been recorded in Perth Here‚Äôs an example, which you can copy, paste and edit into your new script and change the content:\n\n# Week 1 workshop - Getting started\n# Getting familiar with R and load a file\n# Written by Nicholas Wu 06/11/2024 Murdoch University\n\nThe # is to tell R not to run this line of text. i.e., the comment will not be evaluated. Typically, the # is used to annotate or give explanation for what the code is doing.\nLet‚Äôs try typing and running some code:\n\n# Create a variable: number of frogs counted at a pond\nfrogs &lt;- 18\n\n# Add 5 frogs observed the next day\nfrogs_total &lt;- frogs + 5\n\n# Print result\nfrogs_total\n\nHere, you can created a new object frog with the assignment operator ‚Äú&lt;-‚Äù. object_name &lt;- value\nTry this one too:\n\n# Mean leaf size of 5 Eucalyptus trees (cm¬≤)\nleaf_sizes &lt;- c(23.4, 21.8, 25.1, 19.7, 22.5)\n\n# Calculate the average leaf size\nmean(leaf_sizes)\n\nYou can combine multiple elements into a vector with c().\nüß™ Challenge: How would you find the largest leaf size?",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "week1_getting_started.html#step-3-loading-packages-and-files",
    "href": "week1_getting_started.html#step-3-loading-packages-and-files",
    "title": "1¬† Getting Started with R",
    "section": "Step 3: Loading packages and files",
    "text": "Step 3: Loading packages and files\nFirst, we need to\nImagine we‚Äôve collected species abundance data from different habitats. The file is called species_counts.csv.\nFirst, save the file in your RStudio project folder (or Downloads folder for now).\nThe most common packages we will be using in this unit.\nThis is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n# Install packages using the install.packages() function.\ninstall.packages(\"tidyverse\")\n\n# Load the readr package (part of tidyverse)\nlibrary(tidyverse)\nlibrary(readr)\n\nüìù Make sure your working directory is set to the folder where your data file is located. You can use getwd() to check and setwd(‚Äúpath/to/your/folder‚Äù) to change it.\nNote: whenever you see an error like this:\n\nlibrary(ggrepel)\n#&gt; Error in library(ggrepel) : there is no package called ‚Äòggrepel‚Äô\n\nYou need to run install.packages(\"ggrepel\") to install the package.\n\n# Load a CSV file\n#species_data &lt;- read_csv(\"species_counts.csv\")\n\n# View the first few rows\n#head(species_data)",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "week1_getting_started.html#step-3-loading-packages",
    "href": "week1_getting_started.html#step-3-loading-packages",
    "title": "1¬† Getting Started with R",
    "section": "Step 3: Loading packages",
    "text": "Step 3: Loading packages\nThe next few lines of code usually load the packages you will be needing for your analysis. A package is a bundle of commands that can be loaded into R to provide extra functionality. For example, you might load a package for formatting data, or for making maps.\nTo install a package, type install.packages(\"package-name\"). You only need to install packages once, so in this case you can type directly in the console box, rather than saving the line in your script and re-installing the package every time.\nOnce installed, you just need to load the packages using library(package-name). Here, we will load the most common packages we will be using in this unit.\n\n# Install packages using the install.packages() function.\ninstall.packages(\"tidyverse\")\n\n# Load the readr package (part of tidyverse)\nlibrary(tidyverse)\nlibrary(readr)\n\nüìù Make sure your working directory is set to the folder where your data file is located. You can use getwd() to check and setwd(‚Äúpath/to/your/folder‚Äù) to change it.\nNote: whenever you see an error like this:\n\nlibrary(ggrepel)\n#&gt; Error in library(ggrepel) : there is no package called ‚Äòggrepel‚Äô\n\nYou need to run install.packages(\"ggrepel\") to install the package.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "week1_getting_started.html#step-4-loading-files",
    "href": "week1_getting_started.html#step-4-loading-files",
    "title": "1¬† Getting Started with R",
    "section": "Step 4: Loading files",
    "text": "Step 4: Loading files\nImagine we‚Äôve collected species abundance data from different habitats. The file is called species_counts.csv.\nFirst, save the file in your RStudio project folder (or Downloads folder for now).\n\n# Load a CSV file\n#species_data &lt;- read_csv(\"species_counts.csv\")\n\n# View the first few rows\n#head(species_data)",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "week1_getting_started.html#step-2-first-r-commands",
    "href": "week1_getting_started.html#step-2-first-r-commands",
    "title": "1¬† Getting Started with R",
    "section": "Step 2: First R Commands",
    "text": "Step 2: First R Commands\nClick File &gt; New File &gt; R Script to start.\nStart by noting who is writing, the date, and the main goal - in our case, determining how many species from different taxa have been recorded in Perth Here‚Äôs an example, which you can copy, paste and edit into your new script and change the content:\n\n# Week 1 workshop - Getting started\n# Getting familiar with R and load a file\n# Written by Nicholas Wu, 06/11/2024, Murdoch University\n\nThe # is to tell R to not run this line of text. i.e., the comment will not be evaluated. Typically, the # is used to annotate or give explanation for what the code is doing.\nLet‚Äôs try typing and running some code:\n\n# Create a variable: number of frogs counted at a pond\nfrogs &lt;- 18\n\n# Add 5 frogs observed the next day\nfrogs_total &lt;- frogs + 5\n\n# Print result\nfrogs_total\n\nHere, you can created a new object frogs with the assignment operator ‚Äú&lt;-‚Äù. object_name &lt;- value\nTry this one too:\n\n# Mean leaf size of 5 Eucalyptus trees (cm¬≤)\nleaf_sizes &lt;- c(23.4, 21.8, 25.1, 19.7, 22.5)\n\n# Calculate the average leaf size\nmean(leaf_sizes)\n\nYou can combine multiple elements into a vector with c().\nüß™ Challenge: How would you find the largest leaf size?",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "week1_getting_started.html#murdoch-computer",
    "href": "week1_getting_started.html#murdoch-computer",
    "title": "1¬† Getting Started with R",
    "section": "Murdoch computer",
    "text": "Murdoch computer\nAll Murdoch computers have R and Rstudio available. Login to your account and access Rstudio on your work computer.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "week1_getting_started.html#your-own-computer",
    "href": "week1_getting_started.html#your-own-computer",
    "title": "1¬† Getting Started with R",
    "section": "Your own computer",
    "text": "Your own computer\nIf you are using your own personal computer in this unit, here are the steps for downloading and installing R and Rstudio.\n\nInstall R\n\nGo to https://cran.r-project.org\nChoose your operating system and follow the instructions to install.\n\nInstall RStudio\n\nGo to https://posit.co/download/rstudio-desktop/\nDownload the free version of RStudio Desktop.\nInstall it on your computer.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "week2_data_exploration.html#why-is-my-code-not-working",
    "href": "week2_data_exploration.html#why-is-my-code-not-working",
    "title": "2¬† Quality Check",
    "section": "Why is my code not working!",
    "text": "Why is my code not working!\nComputers are only as smart as the humans that use them.\nIf your code is not working there is most likely a spelling mistake or a typographic error. These human errors are easy to miss but equally easy to fix!\nIf your code is not working take a moment to breathe. Then check for common, minor errors such as:\n\nSpelling mistakes\nWrong dataset name\nWrong variable (column) name\nMissing or wrong quotation mark\nMissing bracket\nInconsistent cases (e.g.¬†Uppercase)\nMissed a step\nInvalid syntax (e.g.¬†spaces)\nDuplicates of the same function with multiple errors ‚Äì keep your scripts tidy!\nThese reading/typing mistakes are the majority of encountered errors. They are not a big deal and are easily corrected.\n\nYou can and should easily fix the above mistakes yourself! We want you to know how to work problems out independently.\n\nExercise\nüß™ Why does this code not work?\n\nmy_variable &lt;- 10\nmy_varƒ±able\n#&gt; Error: object 'my_varƒ±able' not found\n\nLook carefully! (This may seem like an exercise in pointlessness, but training your brain to notice even the tiniest difference will pay off when programming.)",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Quality Check</span>"
    ]
  },
  {
    "objectID": "week1_getting_started.html#footnotes",
    "href": "week1_getting_started.html#footnotes",
    "title": "1¬† Quality Check",
    "section": "",
    "text": "A vector is simply a list of items that are of the same type.‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Quality Check</span>"
    ]
  },
  {
    "objectID": "week2_data_exploration.html#before-we-start",
    "href": "week2_data_exploration.html#before-we-start",
    "title": "2¬† Quality Check",
    "section": "",
    "text": "Excerise (5 min)\nüß™ Get your workflow set up\nNext, load your appropriate packages (the same tidyverse, and a one for visualising missing data called visdat at this stage), set your working directory via setwd(), and load the plant_calcium.csv via read_csv().\nLets call the imported data object plant_data.csv this time",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Quality Check</span>"
    ]
  },
  {
    "objectID": "week3_tidy_data.html",
    "href": "week3_tidy_data.html",
    "title": "3¬† Tidy Data",
    "section": "",
    "text": "Background reading\nWeek 3 - Tidying data and clean code\nIn this workshop, you will learn how transform raw data to something usable for analysis, and get familiar with clean code.\nSometimes, it is rare that you get the data in exactly the right form you need from the raw data you import into R. Often you‚Äôll need to create some new variables or summaries, or maybe you just want to rename the variables or reorder the observations in order to make the data a little easier to work with. In this workshop you are going to learn the five key dplyr functions (part of tidyverse) that allow you to solve the vast majority of your data manipulation challenges:\nThese can all be used in conjunction with group_by() which changes the scope of each function from operating on the entire dataset to operating on it group-by-group. These six functions provide the verbs for a language of data manipulation.\nAll verbs work similarly:\nTogether these properties make it easy to chain together multiple simple steps to achieve a complex result. Let‚Äôs dive in and see how these verbs work.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "week3_tidy_data.html#why-is-my-code-not-working",
    "href": "week3_tidy_data.html#why-is-my-code-not-working",
    "title": "3¬† Tidy data",
    "section": "Why is my code not working!",
    "text": "Why is my code not working!\nComputers are only as smart as the humans that use them.\nIf your code is not working there is most likely a spelling mistake or a typographic error. These human errors are easy to miss but equally easy to fix!\nIf your code is not working take a moment to breathe. Then check for common, minor errors such as:\n\nSpelling mistakes\nWrong dataset name\nWrong variable (column) name\nMissing or wrong quotation mark\nMissing bracket\nInconsistent cases (e.g.¬†Uppercase)\nMissed a step\nInvalid syntax (e.g.¬†spaces)\nDuplicates of the same function with multiple errors ‚Äì keep your scripts tidy!\nThese reading/typing mistakes are the majority of encountered errors. They are not a big deal and are easily corrected.\n\nYou can and should easily fix the above mistakes yourself! We want you to know how to work problems out independently.\n\n5.0.1 Excerise\nüß™ Why does this code not work?\n\nmy_variable &lt;- 10\nmy_varƒ±able\n#&gt; Error: object 'my_varƒ±able' not found\n\nLook carefully! (This may seem like an exercise in pointlessness, but training your brain to notice even the tiniest difference will pay off when programming.)\nIn summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tidy data</span>"
    ]
  },
  {
    "objectID": "week2_data_exploration.html#initial-check",
    "href": "week2_data_exploration.html#initial-check",
    "title": "2¬† Quality Check",
    "section": "1.1. Initial check",
    "text": "1.1. Initial check\nThis helps you spot if variables are coded incorrectly (e.g., numbers as characters or vice versa), which could be due to the researcher error before inporting the data. Some example mistakes include:\n\nIncorrect data types: Numeric columns may be read as character (text) if there are unexpected symbols, missing values coded inconsistently, or formatting issues (e.g. a column meant to be numbers may contain text or empty strings, causing R to treat it as character instead of numeric).\nInconsistent coding: Categorical variables may be inconsistently labeled (e.g., ‚ÄúMale‚Äù, ‚Äúmale‚Äù, ‚ÄúM‚Äù), leading to multiple categories that should be merged.\nHeader issues: Sometimes, the header row is missing or duplicated, or extra non-data rows are included at the top or bottom of the file.\nEncoding problems: Special characters (e.g., accents, non-English letters) may not display correctly if the file encoding is not specified or mismatched.\nTrailing spaces: Spaces at the start or end of strings can create unexpected categories or prevent proper matching.\n\n\nFor small datasets (like in this workshop) it is easier to make fixes directly from Excel and import the new .csv file. As before, we will check the data here first with the str() function.\n\n# Check the structure of your data\nstr(plant_data)\n\n# Get a summary of all variables\nsummary(plant_data)\n\n\nExercise (5 min)\nüß™ Find the incorrect class in this dataset\nI have created an example dataset called fake_data with three things wrong with it. Find the the three issues in this dataset.\n\nfake_data &lt;- data.frame(\n  site = c(\"A\", \"B\", \"C\", \"A\", \"B\", \"A\", \"B\", \"C\", \"a\", \"B\", \"A\", \"B\", \"A\", \"B\", \"C\", \"C\", \"B\", \"A\", \"A\", \"C\", \"B\", \"C\", \"C\", \"A\", \"A\", \"A\", \"A\", \"B\", \"C\", \"C\", \"C\"),\n  species_1 = c(7, 25, 30, 50, 38, 13, 5, 31, 34, 22, 41, 25, 22, 37, 39, 13, 45, 10, 40, 7, 28, 36, 19, 2, 12, 5, 16, 38, 27, 44, 44),\n  species_2 = c(9.3, 8.6, 0.8 , 7.5, 9 , 9.1, 4.3, 8.6, 0.1, 7.8, 1.2, 6.2, 0.4, 0.4, 7.4, 1.9, 8.3, 7.8, 2.9, 9.3, 9.1, 2.3, 6.3, 8.5, 6.5, 6.6, \"1,7\", 1.8, 8.4, 4.7, 4.7),\n  species_3 = c(190, 160, 70, 430, 310, 99, 530, 420, 371, 357, 198, 171, 463, 124, 254, 484, 435, 409, 122, 305, 410, 162, 473, 200, 401, 273, 421, 419, 293, 487, 487),\n  species_4 = c(2, 0, 4, 8, 6.7, 3, 7.2, 4.3, 2, NA, 10, 10, 6, 4, 8, 1.7, 1, 0, NA, 4, 6, 8.2, 10, 6, 2, 5, 10, 7, NA, NA, NA)\n)\n\nWhat are the identified issues?",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Quality Check</span>"
    ]
  },
  {
    "objectID": "week2_data_exploration.html#identify-missing-values",
    "href": "week2_data_exploration.html#identify-missing-values",
    "title": "2¬† Quality Check",
    "section": "1.2. Identify missing values",
    "text": "1.2. Identify missing values\nMissing data may appear as blanks, ‚ÄúNA‚Äù, ‚Äú.‚Äù, or other placeholders. If not handled properly, this can cause errors or misinterpretation during analysis.\n\n\n# Check for missing values in each column\ncolSums(is.na(plant_data))\n\n# Visualise missing data pattern\nvisdat::vis_dat(plant_data)\n\nvis_dat() visualises the whole dataframe at once, and provides information about the class of the data input into R, as well as whether the data is missing or not.\nThe function vis_miss() provides a summary of whether the data is missing or not. It also provides the amount of missings in each columns.\n\n# Visualise missing data pattern\nvisdat::vis_miss(plant_data)\n\nOption: Removing columns with missing values\n\n# Removes all rows with NA in the dataset\nplant_data_clean &lt;- plant_data %&gt;% \n  drop_na()\n\n# not recommended if NA is scattered across columns (e.g. leaf1area) because some important variables might be dropped out.\n\n# Drop rows where NA is present in the elevation column only.\nplant_data_clean &lt;- plant_data %&gt;% \n  drop_na(elevation)\n\n# Run the vis_miss again on the clean data to check if the NA rows are removed. \nvisdat::vis_miss(plant_data_clean)\n\n\nExercise (5 min)\nüß™ Identify and handle missing values in the fake_data dataset\n\nWhich column contains missing variables?\nHow many missing values are there?\nRemove rows with missing values and create a new object called fake_data_clean.\nVisualise the new fake_data_clean with visdat::vis_dat(fake_data_clean).",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Quality Check</span>"
    ]
  },
  {
    "objectID": "week2_data_exploration.html#check-for-duplicates",
    "href": "week2_data_exploration.html#check-for-duplicates",
    "title": "2¬† Quality Check",
    "section": "1.3. Check for duplicates",
    "text": "1.3. Check for duplicates\n\nThere are two common duplicate types that can happen in your dataset.\n\nRow duplicates: where the values and names of two rows are identical 99% of the time, this is user error (e.g.¬†copy pasted). These must be identified and removed.\n\n\nsum(duplicated(plant_data))  # Returns the number of duplicate rows\n\n# View the duplicate rows themselves\nplant_data[duplicated(plant_data), ]\n\nIn the plant_data, there are no row duplicates.\n\nValue duplicates: 90% of value duplicates in a column might be biologically realistic responses and is likely due to rounding of values during recording (e.g.¬†values 2.5 and 2.5 might look duplicated due to user rounding. Actual values are 2.47 and 2.49). This is usually okay and can be kept in the dataset. User error occurs if there is a pattern in duplicates (e.g.¬†2,2,6,1,2,2,9,4,2,2,5,9). This can occur due to copy and pasting in the following row by accident.\n\nYou the researcher must decide whether these duplicate responses are real or due to user error.\n\nsum(duplicated(plant_data$leaf_dry_mass))  # Returns the number of duplicate rows\n\nIn the leaf_dry_mass column, there are 51 value duplicates.\n\nExercise (3 min)\nüß™ Identify the duplicate row in the fake_data dataset\n\nWhere is the duplicate row?\nRemove the duplicate row. Either manually in Excel (easy), or an R function (challenge).\nAre there other duplicate values and do you know if they are ecologically relevant duplicates (similar response) or user error?",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Quality Check</span>"
    ]
  },
  {
    "objectID": "week2_data_exploration.html#additional-resource",
    "href": "week2_data_exploration.html#additional-resource",
    "title": "2¬† Quality Check",
    "section": "Additional resource",
    "text": "Additional resource\n\nHow to rename factor levels in R.\nCrashCourse Statistics video on Plots, Outliers, and Justin Timberlake, with examples of went to remove outliers.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Quality Check</span>"
    ]
  },
  {
    "objectID": "week2_data_exploration.html#check-for-outliers",
    "href": "week2_data_exploration.html#check-for-outliers",
    "title": "2¬† Quality Check",
    "section": "2.1 Check for outliers",
    "text": "2.1 Check for outliers\nOutliers aren‚Äôt inherently ‚Äúbad,‚Äù but they can cause problems depending on the context and goals of your analysis. They can:\n\nSkew statistical results: A single extreme value can pull the mean far from the center, increase variability making data seem more spread than it really us, and can distrort the slope and intercept of linear models, leading to misleading predictions.\nCreate obscure patterns: Hide clusters or trends that would otherwise be visible.\nIndicate user error: Data entry mistakes (e.g., typing 1000 instead of 100), measurement errors (e.g., faulty sensors), incorrect data merging (e.g., mixing units like metres and feet).\n\nThere are different ways we can check for outliers. See a more comprehensive page for more options. Here, we will create two common plots to check for outliers, and run some simple methods for detecting and removing outliers.\n\nBoxplot\nWe can use the ggplot() function from the ggplot package to create a boxplot via geom_boxplot() showing the distribution of the lead2area data.\n\n# Boxplot of leaf2area to detect potential outliers\nggplot(plant_data, aes(y = leaf2area)) +\n  geom_boxplot(fill = \"lightgrey\") +\n  theme_bw()\n\nWarning: Removed 117 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\nHistogram\nAnother way we can visualise outliers is with histograms via geom_histogram() or density plots via geom_density()\n\n# Boxplot of height to detect potential outliers\nggplot(plant_data, aes(x = leaf2area)) +\n  geom_histogram() +\n  theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 117 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nHere you can see one clear outlier with a value above 100. All other values are below 25. You can either manually remove the outlier on Excel, or with the following code below.\nOption: Remove outliers using the interquartile range (IQR) rule.\n\n# View potential outliers using IQR rule\nQ1 &lt;- quantile(plant_data$leaf2area, 0.25, na.rm = TRUE)\nQ3 &lt;- quantile(plant_data$leaf2area, 0.75, na.rm = TRUE)\nIQR &lt;- Q3 - Q1\n\n# Filter rows with height outside 1.5 * IQR range\noutliers &lt;- filter(plant, height &lt; (Q1 - 1.5 * IQR) | height &gt; (Q3 + 1.5 * IQR))\noutliers\n\n\n\n\n\n\n\nNote\n\n\n\nDon‚Äôt just blindly remove outliers!\n\n\nMake sure the numbers are biologically realistic based on intuition and comparing with the literature. If the values are unrealistic, it is a safe assumption to remove them.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Quality Check</span>"
    ]
  },
  {
    "objectID": "week2_data_exploration.html#background-reading",
    "href": "week2_data_exploration.html#background-reading",
    "title": "2¬† Quality Check",
    "section": "",
    "text": "Check data structure and types: Use str() and summary() to ensure each column is the correct type (numeric, factor, character, etc.) and matches your expectations. e.g., Convert relevant character columns to factors for categorical analysis.\nInspect for missing values: Use colSums(is.na() or visual tools (e.g., naniar::gg_miss_var() or visdat_viz_dat()) to identify missing data and decide how to handle it.\nCheck for duplicates: Ensure there are no unintended duplicate rows or IDs.\nValidate categorical variables: Confirm that categories are consistent and correctly formatted (e.g., all lower case, no extra spaces).\nScan for outliers or impossible values: Look for values outside expected ranges (e.g., negative stem lengths, impossible dates).\nReview column names: Ensure column names are meaningful, have no spaces or special characters, and are consistent.\nPreview the data: Use plotting functions to visually inspect the data for obvious issues.\nCheck for extra rows or columns: Sometimes, files have summary rows, notes, or empty columns that should be removed.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Quality Check</span>"
    ]
  },
  {
    "objectID": "week2_data_exploration.html#for-the-adventurous-folks",
    "href": "week2_data_exploration.html#for-the-adventurous-folks",
    "title": "2¬† Quality Check",
    "section": "For the adventurous folks",
    "text": "For the adventurous folks\nHere we will start thinking about writing code efficiently. For example, you would have noticed by now that whatever raw data we load in R, any categorical columns are automatically listed as characters. So you can write a code that both loads the data and change the characters to factors in the same line of code (e.g.¬†using the dplyr::mutate() function). Load the plant_calcium_homework.csv and change categorical variables from character and factor.\nRepeat the same homework as above, but instead of correcting the errors by editing them on Excel, do all the corrections within the R environment. Visualise the clean version of the the data with a density plot but separate the density by the treatment groups (reference and W1 variables).",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Quality Check</span>"
    ]
  },
  {
    "objectID": "week2_data_exploration.html#is-the-data-normally-distributed",
    "href": "week2_data_exploration.html#is-the-data-normally-distributed",
    "title": "2¬† Quality Check",
    "section": "2.2 Is the data normally distributed",
    "text": "2.2 Is the data normally distributed\nThis step is required to check if the data is suitable for certain types of analysis. Sometimes, transformation is required for certain data to meet the assumptions of certain statistical methods and improve the interpretability and performance of models. Many statistical techniques‚Äîlike linear regression, ANOVA, and t-tests‚Äîassume that the data is normally distributed, has constant variance, and is linearly related (from your MAS183 - Statistical Data Analysis and MAS224 - Biostatistical Methods units). If the data is skewed, has outliers, or shows non-constant variance (heteroscedasticity), these assumptions are violated, which can lead to biased estimates, incorrect conclusions, or poor model fit.\nBy applying transformations such as logarithmic, square root, or Box-Cox, we can:\n\nReduce skewness and make the distribution more symmetrical.\nStabilise variance across levels of an independent variable.\nImprove linearity between variables.\nMinimise the influence of outliers.\n\nLet‚Äôs check the distribution of leaf2area after removing that one outlier.\n\n # remove values above 100\nplant_data_clean &lt;- plant_data %&gt;%\n  filter(leaf2area &lt; 100)\n\nggplot(plant_data_clean, aes(x = leaf2area)) +\n  geom_histogram() +\n  theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nLooks normally distributed to me! We will learn data transformation in next weeks workshop.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Quality Check</span>"
    ]
  },
  {
    "objectID": "week3_tidy_data.html#step-1-filter-rows-with-filter",
    "href": "week3_tidy_data.html#step-1-filter-rows-with-filter",
    "title": "3¬† Tidy data",
    "section": "3.1 Step 1: Filter rows with filter()",
    "text": "3.1 Step 1: Filter rows with filter()",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tidy data</span>"
    ]
  },
  {
    "objectID": "week3_tidy_data.html#for-the-adventurous-folks",
    "href": "week3_tidy_data.html#for-the-adventurous-folks",
    "title": "3¬† Tidy Data",
    "section": "For the adventurous folks",
    "text": "For the adventurous folks\nYou‚Äôve learned how to use the core dplyr verbs ‚Äî now let‚Äôs practice building more powerful and efficient workflows by combining multiple functions in clean, compact code. Here are your advanced challenges for next week:\n1. Take your cleaned dataset plant_homework_data_clean and write one pipeline that:\n\nFilters only W1 watershed and year 2004.\nCreates a new variable total_mass_g from stem_dry_mass + leaf_dry_mass, multiplied by 1000\nSelects only the new variable and stem_length\nFilters any rows with stem_length greater than 200 mm.\nArranges the result by total_mass_g in descending order.\n\nUse %&gt;% or |&gt; and avoid creating intermediate objects.\n2. Using group_by() and summarise() in a single chain:\n\nGroup the data by watershed and year.\nCalculate the mean, coefficient of variation, and count of total_mass_g.\nArrange by year in ascending order.\n\n3. Write a function that:\n\nTakes a numeric column as input.\nFilters out outliers using the 1.5 √ó IQR rule.\nReturns the filtered data.\n\nTry applying it to leaf_dry_mass and stem_length.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "week3_tidy_data.html#additional-resource",
    "href": "week3_tidy_data.html#additional-resource",
    "title": "3¬† Tidy data",
    "section": "Additional resource",
    "text": "Additional resource\n\nMore comprehensive description of the data transformation functions to explore.\nAn animated version that visualises what some of the tidyverse functions do. Example animated function inner_join()shown below.\n\n\n\n\ninner_join(): All rows from x where there are matching values in y, and all columns from x and y.\n\n\n\n\n\n\nJuice, S. M., Fahey, T. J., Siccama, T. G., Driscoll, C. T., Denny, E. G., Eagar, C., Cleavitt, N. L., Minocha, R. and Richardson, A. D. (2006). Response of sugar maple to calcium addition to northern hardwood forest. Ecology 87, 1267‚Äì1280.\n\n\nPeters, S. C., Blum, J. D., Driscoll, C. T. and Likens, G. E. (2004). Dissolution of wollastonite during the experimental manipulation of hubbard brook watershed 1. Biogeochemistry 67, 309‚Äì329.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tidy data</span>"
    ]
  },
  {
    "objectID": "week3_tidy_data.html#filter-rows-with-filter",
    "href": "week3_tidy_data.html#filter-rows-with-filter",
    "title": "3¬† Tidy data",
    "section": "Filter rows with filter()",
    "text": "Filter rows with filter()\n\nfilter() allows you to subset observations based on their values. The first argument is the name of the data frame. The second and subsequent arguments are the expressions that filter the data frame. For example, we can select all values for the year 2004 for the calcium-treated sites (W1):\n\n# filter function by two columns. A numeric and a categorical.\nfilter(plant_homework_clean_data, year == 2004, watershed == \"W1\")\n\nYou can also filter rows by more one variable in a column using x %in% y logic. This will select every row where x is one of the values in y. For example, we can select all values in transect R1 and W1-1 only:\n\n# filter function for the transect column by \"R1\" and \"W1-1\"\nfilter(plant_homework_clean_data, transect %in% c(\"R1\", \"W1-1\"))\n\nLastly, you can filter by number thresholds. e.g.¬†filtering out extreme values (i.e.¬†for data quality checks), or filter within a range of values. For example, we can select all stem_dry_mass values below 0.9 or between 0.01-0.02:\n\n# filter function by stem_dry_mass values below 0.9\nfilter(plant_homework_clean_data, stem_dry_mass &lt;= 0.9)\n\n# filter function by stem_dry_mass values between 0.01 and 0.02\nfilter(plant_homework_clean_data, stem_dry_mass &gt; 0.01 & stem_dry_mass &lt; 0.02)\n\nOther possible functions and operators within the filter() function can be found here.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tidy data</span>"
    ]
  },
  {
    "objectID": "week3_tidy_data.html#arrange-rows-with-arrange",
    "href": "week3_tidy_data.html#arrange-rows-with-arrange",
    "title": "3¬† Tidy data",
    "section": "Arrange rows with arrange()",
    "text": "Arrange rows with arrange()\n\narrange() works similarly to filter() except that instead of selecting rows, it changes their order. It takes a data frame and a set of column names (or more complicated expressions) to order by. If you provide more than one column name, each additional column will be used to break ties in the values of preceding columns:\n\n# Order the dataset by the ascending stem_length value (lowest in top row to highest in bottom row)\narrange(plant_homework_clean_data, stem_length)\n\n# Order the dataset by the descending stem_length value (highest in top row to lowest in bottom row)\narrange(plant_homework_clean_data, desc(stem_length))\n\nNote: Missing values are always sorted at the end.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tidy data</span>"
    ]
  },
  {
    "objectID": "week3_tidy_data.html#select-columns-with-select",
    "href": "week3_tidy_data.html#select-columns-with-select",
    "title": "3¬† Tidy data",
    "section": "Select columns with select()",
    "text": "Select columns with select()\n\nIt‚Äôs not uncommon to get datasets with hundreds or even thousands of variables. In this case, the first challenge is often narrowing in on the variables you‚Äôre actually interested in. select() allows you to rapidly zoom in on a useful subset using operations based on the names of the variables.\n\n# Select only three varaibles to keep.\nselect(plant_homework_clean_data, year, watershed, stem_dry_mass)",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tidy data</span>"
    ]
  },
  {
    "objectID": "week3_tidy_data.html#add-new-variables-with-mutate",
    "href": "week3_tidy_data.html#add-new-variables-with-mutate",
    "title": "3¬† Tidy data",
    "section": "Add new variables with mutate()",
    "text": "Add new variables with mutate()\n\nBesides selecting sets of existing columns, it‚Äôs often useful to add new columns that are functions of existing columns. That‚Äôs the job of mutate().\nmutate() always adds new columns at the end of your dataset so we‚Äôll start by creating a narrower dataset so we can see the new variables.\n\n# Calculate the total dry mass from leaf_dry_mass and stem_dry_mass and correct the unit from kilogram to gram\nplant_homework_clean_data &lt;- mutate(plant_homework_clean_data,\n                                   total_dry_mass = leaf_dry_mass + stem_dry_mass,\n                                   total_dry_mass_g = total_dry_mass * 1000) # convert kilogram to gram",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tidy data</span>"
    ]
  },
  {
    "objectID": "week3_tidy_data.html#grouped-summaries-with-summarise",
    "href": "week3_tidy_data.html#grouped-summaries-with-summarise",
    "title": "3¬† Tidy data",
    "section": "Grouped summaries with summarise()",
    "text": "Grouped summaries with summarise()\n\nThe last key verb is summarise(). It collapses a data frame to a single row. However, summarise() is not terribly useful unless we pair it with group_by(). This changes the unit of analysis from the complete dataset to individual groups. Then, when you use the dplyr verbs on a grouped data frame they‚Äôll be automatically applied ‚Äúby group‚Äù. For example, if we applied exactly the same code to a data frame (plant_homework_clean_data) grouped by watershed, we get the average values per treatment:\n\n# Get the mean total_dry_mass_g by treatment groups\nplant_homework_clean_data %&gt;%\n  group_by(watershed) %&gt;%\n  summarise(mean = mean(total_dry_mass_g))\n\nWhat happens if we try to summarise a dataset with NA‚Äôs inside?\n\n# Here is an example dataset\nNA_data &lt;- data.frame(site = c(\"A\", \"B\", \"A\", \"B\", \"A\", \"B\"),\n                      species = c(3, 4, 6, 2, 4, NA))\n\n\n# Get the mean species by site\nNA_data %&gt;%\n  group_by(site) %&gt;%\n  summarise(mean = mean(species))\n\n\n# Try this now with na.rm = TRUE\nNA_data %&gt;%\n  group_by(site) %&gt;%\n  summarise(mean = mean(species, na.rm = TRUE))\n\nThat‚Äôs because aggregation functions obey the usual rule of missing values: if there‚Äôs any missing value in the input, the output will be a missing value. Fortunately, all aggregation functions have an na.rm argument which removes the missing values prior to computation. If your dataset has NAs, the na.rm = TRUE argument will be helpful.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tidy data</span>"
    ]
  },
  {
    "objectID": "week3_tidy_data.html#background-reading",
    "href": "week3_tidy_data.html#background-reading",
    "title": "3¬† Tidy Data",
    "section": "",
    "text": "Pick observations by their values (filter()). e.g.¬†keeping rows with values higher than 100.\nReorder the rows (arrange()). e.g.¬†rearrange a numeric column by the largest values.\nPick variables by their names (select()). e.g.¬†keeping selected columns only.\nCreate new variables with functions of existing variables (mutate()). e.g.¬†the sum of values from two numeric columns.\nCollapse many values down to a single summary (summarise()). e.g.¬†getting the overall mean of the dataset.\n\n\n\n\nThe first argument is a data frame.\nThe subsequent arguments describe what to do with the data frame, using the variable names (without quotes).\nThe result is a new data frame.\n\n\n\nStudy context\nIn order to know whether our imported data requires transformation before analysis, we need to know what we are analysing. Let‚Äôs stick with a familiar dataset, the plant_calcium.csv from this study.\nThe plant_calcium.csv dataset contains observations on sugar maple seedlings in untreated and calcium-treated watersheds at Hubbard Brook Experimental Forest in New Hampshire, USA.\nGrowth of sugar maples (Acer saccharum), known for their maple syrup and iconic leaf shape, can be stunted due to soil acidification from prolonged acid rain, which leaches calcium - a nutrient important for plant growth - from soils and stresses maple seedlings.\nTo investigate the impact of soil calcium supplementation on sugar maple seedling growth, researchers at the Hubbard Brook Long Term Ecological Research (LTER) site recorded ‚Äúgeneral sugar maple germinant health by height, leaf area, biomass, and chlorophyll content‚Äù (Juice et al., 2006) for seedlings in untreated and previously calcium-treated watersheds (Peters et al., 2004). By comparing seedling growth in calcium-treated (W1) versus untreated (Reference) watersheds, calcium impacts on sugar maple seedling growth can be explored.\n\n\n\nSugar maple seedling laboratory photos. Photo credit: Tom Siccama.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "week3_tidy_data.html#excerise-10-min",
    "href": "week3_tidy_data.html#excerise-10-min",
    "title": "3¬† Tidy data",
    "section": "Excerise (10 min)",
    "text": "Excerise (10 min)\nüß™ Use the previous functions to complete the following task for the fake_data below\n\nfake_data &lt;- data.frame(\n  site = c(\"A\", \"B\", \"C\", \"A\", \"B\", \"A\", \"B\", \"C\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"C\", \"C\", \"B\", \"A\", \"b\", \"C\", \"B\", \"C\", \"C\", \"A\", \"A\", \"A\", \"A\", \"B\", \"C\", \"C\", \"C\"),\n  species_1 = as.integer(c(7, 25, 30, 50, 38, 13, 5, 31, 34, 22, 41, 25, 22, 37, 39, 13, 45, 10, 40, 7, 28, 36, 19, 2, 12, 5, 16, 38, 27, 44, 44)),\n  species_2 = as.integer(c(9, 8, 1 , 7, 9 , 9, 4, 8, 1, 7, 1, 6, 4, 4, 7, 2, 8, 7, 2, 9, 9, 2, 6, 8, 6, 6, 17, 1, 8, 4, 4)),\n  species_3 = as.integer(c(190, 160, 70, 430, 310, 99, 530, 420, 371, 357, 198, 171, 463, 124, 254, 484, 435, 409, 122, 305, 410, 162, 473, 200, 401, 273, 421, 419, 293, 487, 487)),\n  species_4 = as.integer(c(2, 0, 4, 8, 6, 3, 7, 4, 2, NA, 35, 10, 6, 4, 8, 1, 1, 0, NA, 4, 6, 8, 10, 6, 2, 5, 10, 7, NA, NA, NA))\n  )\n\n\nConduct the usual quality check on this dataset (slightly different mistakes). To save time, there is an inconsistent coding in one of the categorical variables, and an extreme value in one of the numeric variables to remove.\nCreate a new variable column called total_sp that combines all the species.\nSelect only the site and total_sp variable into a new dataframe.\nCalculate the mean, minimum, maximum, and count of total_sp by site.\n\nQuestion:\n\nWhat is the mean total abundance in site B?\nWWhat is the problem with using mean here? Is there a better way to calculate central tendancy for dount data? (hint: think of decimals in this context).\nWhich site has the highest average species abundance?\nWhich site has the most replicate samples?",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tidy data</span>"
    ]
  },
  {
    "objectID": "week3_tidy_data.html#excerise-10-min-1",
    "href": "week3_tidy_data.html#excerise-10-min-1",
    "title": "3¬† Tidy data",
    "section": "Excerise (10 min)",
    "text": "Excerise (10 min)\nüß™ Clean the code below\nYour colleague got a new job and you have been promoted to his position where you are taking over his unfinished projects. The ecological data has been collected and some quick analysis was performed. But, you noticed that he didnt perform a quality check on the raw data and the # explanations are not clear. The current state is not suitable for the client so you are assigned to clean\nClean the code where relevent, and check if all the code is working on your end. Note, change the working directory to your folder and load the xxx to work on.\n\n# Exercise 2\n# Cleaning code and tidying dataset\n# John Smith, 25-03/2025\n\n\n## Set up ## ----------------------------------------------------------------\n\nlibrary(tiderVerse)\n\n# Set my directory\nsetwd(\"C:/User/CC-1-RBasics-master\")\n\n## Loading and cleaning data ## ---------------------------------------------\n\n# Load raw data\nplant_calcium_data &lt;- read_csv(\"C:/Users/75002992/OneDrive - Murdoch University/Teaching/ECS200/ECS200 - Workshop/data/plant_calcium.csv\")\n\n\n\n\n## Analysis ## -------------------------------------------------------------\n\n# All code involved with preliminary analysis, full analysis, and model output\n\n## Visualisation ## --------------------------------------------------------\n\n# Creating beautiful figures for your report!\n\n\n\n\nfake_data &lt;- data.frame(\n  site = c(\"A\", \"B\", \"C\", \"A\", \"B\", \"A\", \"B\", \"C\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"C\", \"C\", \"B\", \"A\", \"b\", \"C\", \"B\", \"C\", \"C\", \"A\", \"A\", \"A\", \"A\", \"B\", \"C\", \"C\", \"C\"),\n  species_1 = as.integer(c(7, 25, 30, 50, 38, 13, 5, 31, 34, 22, 41, 25, 22, 37, 39, 13, 45, 10, 40, 7, 28, 36, 19, 2, 12, 5, 16, 38, 27, 44, 44)),\n  species_2 = as.integer(c(9, 8, 1 , 7, 9 , 9, 4, 8, 1, 7, 1, 6, 4, 4, 7, 2, 8, 7, 2, 9, 9, 2, 6, 8, 6, 6, 17, 1, 8, 4, 4)),\n  species_3 = as.integer(c(190, 160, 70, 430, 310, 99, 530, 420, 371, 357, 198, 171, 463, 124, 254, 484, 435, 409, 122, 305, 410, 162, 473, 200, 401, 273, 421, 419, 293, 487, 487)),\n  species_4 = as.integer(c(2, 0, 4, 8, 6, 3, 7, 4, 2, NA, 35, 10, 6, 4, 8, 1, 1, 0, NA, 4, 6, 8, 10, 6, 2, 5, 10, 7, NA, NA, NA))\n  )",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tidy data</span>"
    ]
  },
  {
    "objectID": "week3_tidy_data.html#file-names",
    "href": "week3_tidy_data.html#file-names",
    "title": "3¬† Tidy data",
    "section": "File names",
    "text": "File names\nFile names should be machine readable: avoid spaces, symbols, and special characters. Prefer file names that are all lower case, and never have names that differ only in their capitalisation. Delimit words with - or _. Use .R as the extension of R files.\n\n# Good\nfit_models.R\nutility_functions.R\nexploratory-data-analysis.R\n\n# Bad\nfit models.R\nfoo.r\nExploratoryDataAnaylsis.r\n\nFile names should be human readable: use file names to describe what‚Äôs in the file.\n\n# Good\nreport-draft-notes.txt\n\n# Bad\ntemp.r\n\nUse the same structure for closely related files:\n\n# Good\nfig-eda.png\nfig-model-3.png\n\n# Bad\nfigure eda.PNG\nfig model three.png",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tidy data</span>"
    ]
  },
  {
    "objectID": "week3_tidy_data.html#r-organisation",
    "href": "week3_tidy_data.html#r-organisation",
    "title": "3¬† Tidy data",
    "section": "R organisation",
    "text": "R organisation\nUse commented lines of - to break up your file into easily readable chunks. Here is a template you can use for your future projects.\n\n# Title.\n# Short description of what is being done.\n# Authors name and date of analysis.\n\n\n## Set up ## ----------------------------------------------------------------\n\n# any code related to setting up directory, installing and loading packages (e.g. from workshop 1)\n\n## Loading and cleaning data ## ---------------------------------------------\n\n# any code relating to loading the data, checking the quality, transforming the dataset, so that it is ready for analysis\n\n\n## Analysis ## -------------------------------------------------------------\n\n# All code involved with preliminary analysis, full analysis, and model output\n\n## Visualisation ## --------------------------------------------------------\n\n# Creating beautiful figures for your report!",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tidy data</span>"
    ]
  },
  {
    "objectID": "week3_tidy_data.html#syntax",
    "href": "week3_tidy_data.html#syntax",
    "title": "3¬† Tidy data",
    "section": "Syntax",
    "text": "Syntax\nVariable and function names should use only lowercase letters, numbers, and . Use underscores () (so called snake case) to separate words within a name.\n\n# Good\nday_one\nday_1\n\n# Bad\nDayOne\ndayone\n\nGenerally, variable names should be nouns and function names should be verbs. Strive for names that are concise and meaningful (this is not easy!).\n\n# Good\nday_one\n\n# Bad\nfirst_day_of_the_month\ndjm1\n\nWhere possible, avoid re-using names of common functions and variables. This will cause confusion for the readers of your code.\n\n# Bad\nT &lt;- FALSE\nc &lt;- 10\nmean &lt;- function(x) sum(x)",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tidy data</span>"
    ]
  },
  {
    "objectID": "week3_tidy_data.html#exercise-10-min",
    "href": "week3_tidy_data.html#exercise-10-min",
    "title": "3¬† Tidy Data",
    "section": "Exercise (10 min)",
    "text": "Exercise (10 min)\nüß™ Use the previous functions to complete the following task for the fake_data below\n\nfake_data &lt;- data.frame(\n  site = c(\"A\", \"B\", \"C\", \"A\", \"B\", \"A\", \"B\", \"C\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"C\", \"C\", \"B\", \"A\", \"b\", \"C\", \"B\", \"C\", \"C\", \"A\", \"A\", \"A\", \"A\", \"B\", \"C\", \"C\", \"C\"),\n  species_1 = as.integer(c(7, 25, 30, 50, 38, 13, 5, 31, 34, 22, 41, 25, 22, 37, 39, 13, 45, 10, 40, 7, 28, 36, 19, 2, 12, 5, 16, 38, 27, 44, 44)),\n  species_2 = as.integer(c(9, 8, 1 , 7, 9 , 9, 4, 8, 1, 7, 1, 6, 4, 4, 7, 2, 8, 7, 2, 9, 9, 2, 6, 8, 6, 6, 17, 1, 8, 4, 4)),\n  species_3 = as.integer(c(190, 160, 70, 430, 310, 99, 530, 420, 371, 357, 198, 171, 463, 124, 254, 484, 435, 409, 122, 305, 410, 162, 473, 200, 401, 273, 421, 419, 293, 487, 487)),\n  species_4 = as.integer(c(2, 0, 4, 8, 6, 3, 7, 4, 2, NA, 35, 10, 6, 4, 8, 1, 1, 0, NA, 4, 6, 8, 10, 6, 2, 5, 10, 7, NA, NA, NA))\n  )\n\n\nConduct the usual quality check on this dataset (slightly different mistakes). To save time, there is an inconsistent coding in one of the categorical variables, and an extreme value in one of the numeric variables to remove.\nCreate a new variable column called total_sp that combines all the species.\nSelect only the site and total_sp variable into a new dataframe.\nCalculate the mean, minimum, maximum, and count of total_sp by site.\n\nQuestion:\n\nWhat is the mean total abundance in site B?\nWWhat is the problem with using mean here? Is there a better way to calculate central tendancy for dount data? (hint: think of decimals in this context).\nWhich site has the highest average species abundance?\nWhich site has the most replicate samples?",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "week3_tidy_data.html#exercise-10-min-1",
    "href": "week3_tidy_data.html#exercise-10-min-1",
    "title": "3¬† Tidy data",
    "section": "Exercise (10 min)",
    "text": "Exercise (10 min)\nüß™ Clean the code below\nYour colleague got a new job and you have been promoted to his position where you are taking over his unfinished projects. The ecological data has been collected and some quick analysis was performed. But, you noticed that he didnt perform a quality check on the raw data and the # explanations are not clear. The current state is not suitable for the client so you are assigned to clean.\nClean the code where relevent, and check if all the code is working on your end.\nNote: change the working directory to your folder and load the plant_calcium.csv to work on. Note 2: The same errors to fix are from the week 2 - data quality workshop.\n\n# Exercise 2\n# Cleaning code and tidying dataset\n# John Smith, 25-03/2025\n\n## Set up ## ----------------------------------------------------------------\nlibrary(tiderVerse)\n\n# Set my directory\nsetwd(\"your working directory\")\n\n## Loading and cleaning data ## ---------------------------------------------\n\n# Load raw data\nplant_calcium data &lt;- read_csv(\"data/plant_calcium.csv\")\nstr(plant_data)\n\n\n## Analysis ## -------------------------------------------------------------\n\n# Fit model: height explained by elevation\nmodel &lt;- lm(height ~ elevation, data = plant_data_clean)\n\n# Check model summary\nsummary(model)\n\n# All code involved with preliminary analysis, full analysis, and model output\n\n## Visualisation ## --------------------------------------------------------\n\n# Creating beautiful figures for your report!",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tidy data</span>"
    ]
  },
  {
    "objectID": "week3_tidy_data.html#exercise-15-min",
    "href": "week3_tidy_data.html#exercise-15-min",
    "title": "3¬† Tidy Data",
    "section": "Exercise (15 min)",
    "text": "Exercise (15 min)\nüß™ Clean the code below\nYour colleague got a new job and you have been promoted to his position where you are taking over his unfinished projects. The ecological data has been collected and some quick analysis was performed. But, you noticed that he didnt perform a quality check on the raw data and the # explanations are not clear. The current state is not suitable for the client so you are assigned to clean.\nClean the code where relevent, and check if all the code is working on your end.\nNote: change the working directory to your folder and load the plant_calcium.csv to work on. Note 2: The same errors to fix are from the week 2 - data quality workshop.\n\n# Exercise 2\n# Cleaning code and tidying dataset\n# John Smith, 25-03/2025\n\n## Set up ## ----------------------------------------------------------------\nlibrary(tiderVerse)\n\n# Set my directory\nsetwd(\"your working directory\")\n\n## Loading and cleaning data ## ---------------------------------------------\n\n# Load raw data\nplant data &lt;- read_csv(\"data/plant_calcium.csv\")\nstr(plant data)\n\n## Analysis ## -------------------------------------------------------------\n\n# Fit model: height explained by elevation\nmodel &lt;- lm(stem_dry_mass ~ stem_length, data = plant_data)\n\n# Check model summary\nsummary(model)\n\n## Visualisation ## --------------------------------------------------------\n\n# Creating scatterplot figure\nplant_data %&gt;%\n  ggplot() +\n  geom_point(aes(x = stem_length, y = Stem_dry_mass)) +\n  labs(\n    x = \"Stem Length (mm)\",\n    y = \"Stem Dry Mass (g)\",\n    title = \"Stem Dry Mass vs. Stem Length in Sugar Maple Seedlings\",\n    subtitle = \"Hubbard Brook LTER\"\n  ) +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "week4_descriptive_stats.html",
    "href": "week4_descriptive_stats.html",
    "title": "4¬† Descriptive Stats",
    "section": "",
    "text": "Background reading\nWeek 4 - Descriptive stats and plotting data\nIn this workshop, you will learn how to summarise different types of ecological data using appropriate statistical measures and visualisations. Understanding the right measures of central tendency and spread is critical before conducting any formal hypothesis testing.\nBefore we can decide on the appropriate descriptive stats it helps to understand the scale of your variable. The table below outlines appropriate measures for both qualitative and quantitative variables:\nWhen laying out your datasheet for your study, it is important to know what data types each column will be when imported into R. Here is an example data sheet that is color-coded according to the type of variable: nominal, continuous, ordinal, and binary.\nKey Terms used in this workshop:\nIf you are still confused by the terminologies, the CrashCourse Statistics videos on the Mean, Median, and Mode: Measures of Central Tendency (11:22 min long), and the Measures of Spread (11:46 min long) better explains the terms visually.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Descriptive Stats</span>"
    ]
  },
  {
    "objectID": "week4_descriptive_stats.html#background-reading",
    "href": "week4_descriptive_stats.html#background-reading",
    "title": "4¬† Descriptive Stats",
    "section": "",
    "text": "Data Scale\nType\nDefinition\nExamples\nCentral Tendency\nSpread/Variation\n\n\n\n\nNominal\nQualitative\nNamed categories with no intrinsic order\nsex (male/female), species name\nMode\nCount, Proportion\n\n\nOrdinal\nQualitative\nNamed categories data with a natural order\nlife stage (egg, juvenile, adult)\nMedian, Mode\nRange, IQR\n\n\nInterval\nQuantitative\nEqual intervals between values but no true zero\ntemperature, pH, dates\nMean, Median\nSD, SE, CI\n\n\nRatio\nQuantitative\nEqual intervals and a true zero\nlength, age, body mass\nMean, Median\nSD, SE, CV, IQR, CI, variance\n\n\n\n\n\n\n\nMean: Average value. Use with symmetric, normally distributed data.\nMedian: Middle value. More robust to outliers and skewed data.\nMode: Most frequent value (useful for categories).\nStandard Deviation (SD): How spread out the data is.\nStandard Error (SE): SD divided by the square root of n (how precise the mean is).\nCoefficient of Variation (CV): SD / Mean. Compares spread across variables.\nInterquartile Range (IQR): Range between 25th and 75th percentiles.\nConfidence Interval (CI): A range that likely contains the true mean.\nVariance: How much values differ from the average value.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Descriptive Stats</span>"
    ]
  },
  {
    "objectID": "week4_descriptive_stats.html#exercise-10-min",
    "href": "week4_descriptive_stats.html#exercise-10-min",
    "title": "4¬† Descriptive Stats",
    "section": "Exercise (10 min)",
    "text": "Exercise (10 min)\nüß™ Use the previous functions to complete the following task for the fake_data below\n\nfake_data &lt;- data.frame(\n  site = c(\"A\", \"B\", \"C\", \"A\", \"B\", \"A\", \"B\", \"C\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"C\", \"C\", \"B\", \"A\", \"b\", \"C\", \"B\", \"C\", \"C\", \"A\", \"A\", \"A\", \"A\", \"B\", \"C\", \"C\", \"C\"),\n  species_1 = as.integer(c(7, 25, 30, 50, 38, 13, 5, 31, 34, 22, 41, 25, 22, 37, 39, 13, 45, 10, 40, 7, 28, 36, 19, 2, 12, 5, 16, 38, 27, 44, 44)),\n  species_2 = as.integer(c(9, 8, 1 , 7, 9 , 9, 4, 8, 1, 7, 1, 6, 4, 4, 7, 2, 8, 7, 2, 9, 9, 2, 6, 8, 6, 6, 17, 1, 8, 4, 4)),\n  species_3 = as.integer(c(190, 160, 70, 430, 310, 99, 530, 420, 371, 357, 198, 171, 463, 124, 254, 484, 435, 409, 122, 305, 410, 162, 473, 200, 401, 273, 421, 419, 293, 487, 487)),\n  species_4 = as.integer(c(2, 0, 4, 8, 6, 3, 7, 4, 2, NA, 35, 10, 6, 4, 8, 1, 1, 0, NA, 4, 6, 8, 10, 6, 2, 5, 10, 7, NA, NA, NA))\n  )\n\n\nConduct the usual quality check on this dataset (slightly different mistakes). To save time, there is an inconsistent coding in one of the categorical variables, and an extreme value in one of the numeric variables to remove.\nCreate a new variable column called total_sp that combines all the species.\nSelect only the site and total_sp variable into a new dataframe.\nCalculate the mean, minimum, maximum, and count of total_sp by site.\n\nQuestion:\n\nWhat is the mean total abundance in site B?\nWWhat is the problem with using mean here? Is there a better way to calculate central tendancy for dount data? (hint: think of decimals in this context).\nWhich site has the highest average species abundance?\nWhich site has the most replicate samples?",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Descriptive Stats</span>"
    ]
  },
  {
    "objectID": "week4_descriptive_stats.html#exercise-15-min",
    "href": "week4_descriptive_stats.html#exercise-15-min",
    "title": "4¬† Descriptive Stats",
    "section": "Exercise (15 min)",
    "text": "Exercise (15 min)\nüß™ Clean the code below\nYour colleague got a new job and you have been promoted to his position where you are taking over his unfinished projects. The ecological data has been collected and some quick analysis was performed. But, you noticed that he didnt perform a quality check on the raw data and the # explanations are not clear. The current state is not suitable for the client so you are assigned to clean.\nClean the code where relevent, and check if all the code is working on your end.\nNote: change the working directory to your folder and load the plant_calcium.csv to work on. Note 2: The same errors to fix are from the week 2 - data quality workshop.\n\n# Exercise 2\n# Cleaning code and tidying dataset\n# John Smith, 25-03/2025\n\n## Set up ## ----------------------------------------------------------------\nlibrary(tiderVerse)\n\n# Set my directory\nsetwd(\"your working directory\")\n\n## Loading and cleaning data ## ---------------------------------------------\n\n# Load raw data\nplant data &lt;- read_csv(\"data/plant_calcium.csv\")\nstr(plant data)\n\n## Analysis ## -------------------------------------------------------------\n\n# Fit model: height explained by elevation\nmodel &lt;- lm(stem_dry_mass ~ stem_length, data = plant_data)\n\n# Check model summary\nsummary(model)\n\n## Visualisation ## --------------------------------------------------------\n\n# Creating scatterplot figure\nplant_data %&gt;%\n  ggplot() +\n  geom_point(aes(x = stem_length, y = Stem_dry_mass)) +\n  labs(\n    x = \"Stem Length (mm)\",\n    y = \"Stem Dry Mass (g)\",\n    title = \"Stem Dry Mass vs. Stem Length in Sugar Maple Seedlings\",\n    subtitle = \"Hubbard Brook LTER\"\n  ) +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Descriptive Stats</span>"
    ]
  },
  {
    "objectID": "week4_descriptive_stats.html#for-the-adventurous-folks",
    "href": "week4_descriptive_stats.html#for-the-adventurous-folks",
    "title": "4¬† Descriptive Stats",
    "section": "For the adventurous folks",
    "text": "For the adventurous folks\nWe will use a new dataset provided by lterdatasampler, called pie_crab. The pie_crab contains body size data of Atlantic marsh fiddler crab (Minuca pugnax) across various salt marshes throughout the eastern coast of the United States to test the Bergmann‚Äôs rule, which predicts that organisms at higher latitudes are larger than ones at lower latitudes.\n\n13 marshes were sampled on the Atlantic coast of the United States in summer 2016, spanning &gt; 12 degrees of latitude, from northeast Florida to northeast Massachusetts. Between 25 and 37 adult male fiddler crabs were collected, and their sizes recorded, from each marsh between 2016-07-24 and 2016-08-13.\nHere is the metadata of the pie_crab data: - date: Date of collection - latitude: Latitude of the collection site in degrees - site: The site ID - size: Mean carapace width (mm) - air_temp: The mean air temperature of the day. - air_temp_sd: The standard deviation of the measured air temprature. Also an indicator of seasonality - water_temp - water_temp_sd - name: Names of the sampled Marshes.\n\n#install.packages(\"lterdatasampler\")\n\n# Load R package\nlibrary(lterdatasampler)\n\n# Load 'pie_crab' from the R package\nglimpse(pie_crab)\n\nRows: 392\nColumns: 9\n$ date          &lt;date&gt; 2016-07-24, 2016-07-24, 2016-07-24, 2016-07-24, 2016-07‚Ä¶\n$ latitude      &lt;dbl&gt; 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, ‚Ä¶\n$ site          &lt;chr&gt; \"GTM\", \"GTM\", \"GTM\", \"GTM\", \"GTM\", \"GTM\", \"GTM\", \"GTM\", ‚Ä¶\n$ size          &lt;dbl&gt; 12.43, 14.18, 14.52, 12.94, 12.45, 12.99, 10.32, 11.19, ‚Ä¶\n$ air_temp      &lt;dbl&gt; 21.792, 21.792, 21.792, 21.792, 21.792, 21.792, 21.792, ‚Ä¶\n$ air_temp_sd   &lt;dbl&gt; 6.391, 6.391, 6.391, 6.391, 6.391, 6.391, 6.391, 6.391, ‚Ä¶\n$ water_temp    &lt;dbl&gt; 24.502, 24.502, 24.502, 24.502, 24.502, 24.502, 24.502, ‚Ä¶\n$ water_temp_sd &lt;dbl&gt; 6.121, 6.121, 6.121, 6.121, 6.121, 6.121, 6.121, 6.121, ‚Ä¶\n$ name          &lt;chr&gt; \"Guana Tolomoto Matanzas NERR\", \"Guana Tolomoto Matanzas‚Ä¶\n\n\n1. Do a data quality check\n\nAny missing data?\nAre there duplicates?\nCheck the data structure and types and correct any mistakes\nCheck the data distribution carapace size. Are there outliers?\n\n2. Calculate the central tendency, spread, and sample size of the carapace size size by site and latitude\n\nDecide on calculations of central tendency and spread you will apply based on the distribution observed.\nWhich site has the highest average carapace size?\nWhich site has the lowest sample size?\n\n3. Plot the relationship between latitude and mean carapace width (plus raw carapace width) to test the Bergmann‚Äôs rule in Fidler crabs\n\nMake a publication ready figure (include colour).\nWhat relationship can you see?",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Descriptive Stats</span>"
    ]
  },
  {
    "objectID": "week3_diversity_index.html",
    "href": "week3_diversity_index.html",
    "title": "3¬† Diversity Indices",
    "section": "",
    "text": "Background reading\nWeek 3 - Measurements of biodiversity\nIn this workshop, you will learn different types of ways to quantify diversity and abundance in your dataset. This section is particularly important for groups who are doing field studies, but this quantitative skill is also useful if you have a job in field ecology.\nBiodiversity can be measured using different scales and metrics, most commonly by looking at species richness (the number of different species) and species evenness (the relative proportion of individuals in each species). Other measurements include alpha, beta, and gamma diversity, which describe diversity within a single site, between sites, and across a larger region, respectively. Here, we will focus on the most common ways of quantifying diversity and abundance.\nAbundance metrics\nSpecies richness metrics\nDiversity indices\nOther types of diversity metrics such as R√©nyi and Tsallis diversities, taxonomic diversity and taxonomic distinctness, alpha and beta diveristy can also be done with the vegan R package.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Diversity Indices</span>"
    ]
  },
  {
    "objectID": "week3_diversity_index.html#background-reading",
    "href": "week3_diversity_index.html#background-reading",
    "title": "3¬† Diversity Indices",
    "section": "",
    "text": "Absolute abundance (\\(N_i\\)): The total number of individuals of a species recorded. A simple and intuitive calculation, but sensitive to sampling effort, i.e, cannot compare across surveys without standardisation. Only use when sampling effort is standardised (e.g., same transect length, trap nights).\nRelative abundance (\\(p_i\\)): Comparable across communities and is foundational for many diversity indices. Only use when communities have differing total abundance, or you need proportional representation, or over time.\nDensity (\\(d_i\\); individuals per area or effort): Allows spatial comparison, but requires accurate area/effort estimates. Appropriate for quadrat/plot/transect sampling.\n\n\n\nSpecies richness (\\(S\\)): The total number of unique species. A sample and intuitive calculation, but highly sensitive to sample size and rare species. Only use when comparing equally-sampled plots or as a component in more complex metrics.\nRarefied richness: Standardised species richness for a fixed number of individuals or samples. Allows fair comparison across unequal sampling effort, but assumes random sampling, i.e low resolution when sample sizes differ greatly. Only use in surveys with unequal effort.\n\n\n\nShannon diversity index (\\(H'\\)): Incorporates both species richness and evenness (the relative abundance of each species). Widely used, but difficult to interpret ecologically. Moderately sensitive to rare species, but good for comparing diversity across sites or time when both abundance and richness matter.\nSimpson‚Äôs diversity index (\\(D\\)): Emphasises the dominance of species and is less sensitive to rare species. However, underweights rare species and may mask losses in low-abundance taxa. Use in management-focused studies (dominance effects), and stable community comparisons.\n\n\n\nNew functions\nHere are some new functions from the tidyverse you will be using in this workshop.\n\npivot_longer() converts data from wide format to long format.\nstarts_with()*used inside select() to choose columns whose names begin with a specific string.\nacross()* applies a function to multiple columns in a summarise() or mutate() call. e.g.¬†everything() = select all columns and sum = function to apply.\n.x* is a pronoun used inside anonymous functions in tidyverse pipelines‚Äîespecially in across() and map(). It represents the current column being processed.\n\n‚Äô*‚Äô See ‚ÄòRelative abundance‚Äô section for example use.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Diversity Indices</span>"
    ]
  },
  {
    "objectID": "week3_diversity_index.html#exercise-10-min",
    "href": "week3_diversity_index.html#exercise-10-min",
    "title": "3¬† Diversity Indices",
    "section": "Exercise (10 min)",
    "text": "Exercise (10 min)\nüß™ Use the previous R code and functions from the previous week to complete the following task for the fake_data_clean below",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Diversity Indices</span>"
    ]
  },
  {
    "objectID": "week3_diversity_index.html#exercise-15-min",
    "href": "week3_diversity_index.html#exercise-15-min",
    "title": "3¬† Tidy Data",
    "section": "Exercise (15 min)",
    "text": "Exercise (15 min)\nüß™ Clean the code below\nYour colleague got a new job and you have been promoted to his position where you are taking over his unfinished projects. The ecological data has been collected and some quick analysis was performed. But, you noticed that he didnt perform a quality check on the raw data and the # explanations are not clear. The current state is not suitable for the client so you are assigned to clean.\nClean the code where relevent, and check if all the code is working on your end.\nNote: change the working directory to your folder and load the plant_calcium.csv to work on. Note 2: The same errors to fix are from the week 2 - data quality workshop.\n\n# Exercise 2\n# Cleaning code and tidying dataset\n# John Smith, 25-03/2025\n\n## Set up ## ----------------------------------------------------------------\nlibrary(tiderVerse)\n\n# Set my directory\nsetwd(\"your working directory\")\n\n## Loading and cleaning data ## ---------------------------------------------\n\n# Load raw data\nplant data &lt;- read_csv(\"data/plant_calcium.csv\")\nstr(plant data)\n\n## Analysis ## -------------------------------------------------------------\n\n# Fit model: height explained by elevation\nmodel &lt;- lm(stem_dry_mass ~ stem_length, data = plant_data)\n\n# Check model summary\nsummary(model)\n\n## Visualisation ## --------------------------------------------------------\n\n# Creating scatterplot figure\nplant_data %&gt;%\n  ggplot() +\n  geom_point(aes(x = stem_length, y = Stem_dry_mass)) +\n  labs(\n    x = \"Stem Length (mm)\",\n    y = \"Stem Dry Mass (g)\",\n    title = \"Stem Dry Mass vs. Stem Length in Sugar Maple Seedlings\",\n    subtitle = \"Hubbard Brook LTER\"\n  ) +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "week3_diversity_index.html#for-the-adventurous-folks",
    "href": "week3_diversity_index.html#for-the-adventurous-folks",
    "title": "3¬† Diversity Indices",
    "section": "For the adventurous folks",
    "text": "For the adventurous folks\nTry to get Shannon diversity index by treatment and block. Convert wide data to sums per treatment x block.\nSee if you can replicate this figure.\n\nshannon_inv %&gt;%\n  ggplot() +\n  geom_point(aes(x = treatment, y = shannon), size = 2) +\n  labs(x = \"Treatment\", y = \"Shannon diveristy index (H')\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nThis is more reflective of the normal variation across experimental blocks and where your report should be going towards instead ot the mean only.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Diversity Indices</span>"
    ]
  },
  {
    "objectID": "week1_getting_started.html#background-reading",
    "href": "week1_getting_started.html#background-reading",
    "title": "1¬† Quality Check",
    "section": "Background reading",
    "text": "Background reading\nHow do we know the data is ready for analysis? Before we do any analysis, we need to check the raw data for the following:\n\nCheck data structure and types: Use str() and summary() to ensure each column is the correct type (numeric, factor, character, etc.) and matches your expectations. e.g., Convert relevant character columns to factors for categorical analysis.\nInspect for missing values: Use colSums(is.na() or visual tools (e.g., naniar::gg_miss_var() or visdat_viz_dat()) to identify missing data and decide how to handle it.\nCheck for duplicates: Ensure there are no unintended duplicate rows or IDs.\nValidate categorical variables: Confirm that categories are consistent and correctly formatted (e.g., all lower case, no extra spaces).\nScan for outliers or impossible values: Look for values outside expected ranges (e.g., negative stem lengths, impossible dates).\nReview column names: Ensure column names are meaningful, have no spaces or special characters, and are consistent.\nPreview the data: Use plotting functions to visually inspect the data for obvious issues.\nCheck for extra rows or columns: Sometimes, files have summary rows, notes, or empty columns that should be removed.\n\nThese steps help ensure your data is reliable, interpretable, and suitable for statistical analysis. Importantly, it makes you a trustworthy researcher!",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Quality Check</span>"
    ]
  },
  {
    "objectID": "week1_getting_started.html#initial-check",
    "href": "week1_getting_started.html#initial-check",
    "title": "1¬† Quality Check",
    "section": "1.1. Initial check",
    "text": "1.1. Initial check\nThis helps you spot if variables are coded incorrectly (e.g., numbers as characters or vice versa), which could be due to the researcher error before inporting the data. Some example mistakes include:\n\nIncorrect data types: Numeric columns may be read as character (text) if there are unexpected symbols, missing values coded inconsistently, or formatting issues (e.g. a column meant to be numbers may contain text or empty strings, causing R to treat it as character instead of numeric).\nInconsistent coding: Categorical variables may be inconsistently labeled (e.g., ‚ÄúMale‚Äù, ‚Äúmale‚Äù, ‚ÄúM‚Äù), leading to multiple categories that should be merged.\nHeader issues: Sometimes, the header row is missing or duplicated, or extra non-data rows are included at the top or bottom of the file.\nEncoding problems: Special characters (e.g., accents, non-English letters) may not display correctly if the file encoding is not specified or mismatched.\nTrailing spaces: Spaces at the start or end of strings can create unexpected categories or prevent proper matching.\n\n\nFor small datasets (like in this workshop) it is easier to make fixes directly from Excel and import the new .csv file. As before, we will check the data here first with the str() function.\n\n# Check the structure of your data\nstr(plant_data)\n\n# Get a summary of all variables\nsummary(plant_data)\n\n\nExercise (5 min)\nüß™ Find the incorrect class in this dataset\nI have created an example dataset called fake_data with three things wrong with it. Find the three issues in this dataset.\n\nfake_data &lt;- data.frame(\n  site = c(\"A\", \"B\", \"C\", \"A\", \"B\", \"A\", \"B\", \"C\", \"a\", \"B\", \"A\", \"B\", \"A\", \"B\", \"C\", \"C\", \"B\", \"A\", \"A\", \"C\", \"B\", \"C\", \"C\", \"A\", \"A\", \"A\", \"A\", \"B\", \"C\", \"C\", \"C\"),\n  species_1 = c(7, 25, 30, 50, 38, 13, 5, 31, 34, 22, 41, 25, 22, 37, 39, 13, 45, 10, 40, 7, 28, 36, 19, 2, 12, 5, 16, 38, 27, 44, 44),\n  species_2 = c(9.3, 8.6, 0.8 , 7.5, 9 , 9.1, 4.3, 8.6, 0.1, 7.8, 1.2, 6.2, 0.4, 0.4, 7.4, 1.9, 8.3, 7.8, 2.9, 9.3, 9.1, 2.3, 6.3, 8.5, 6.5, 6.6, \"1,7\", 1.8, 8.4, 4.7, 4.7),\n  species_3 = c(190, 160, 70, 430, 310, 99, 530, 420, 371, 357, 198, 171, 463, 124, 254, 484, 435, 409, 122, 305, 410, 162, 473, 200, 401, 273, 421, 419, 293, 487, 487),\n  species_4 = c(2, 0, 4, 8, 6.7, 3, 7.2, 4.3, 2, NA, 10, 10, 6, 4, 8, 1.7, 1, 0, NA, 4, 6, 8.2, 10, 6, 2, 5, 10, 7, NA, NA, NA)\n)\n\nWhat are the identified issues?\n\n\nShow answer\n\n\nsite should be factor, not character\nspecies_2 should be numeric, not character\nspecies_4 has 5 NA‚Äôs",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Quality Check</span>"
    ]
  },
  {
    "objectID": "week1_getting_started.html#identify-missing-values",
    "href": "week1_getting_started.html#identify-missing-values",
    "title": "1¬† Quality Check",
    "section": "1.2. Identify missing values",
    "text": "1.2. Identify missing values\nMissing data may appear as blanks, ‚ÄúNA‚Äù, ‚Äú.‚Äù, or other placeholders. If not handled properly, this can cause errors or misinterpretation during analysis.\n\n\n# Check for missing values in each column\ncolSums(is.na(plant_data))\n\n# Visualise missing data pattern\nvisdat::vis_dat(plant_data)\n\nvis_dat() visualises the whole dataframe at once, and provides information about the class of the data input into R, as well as whether the data is missing or not.\nThe function vis_miss() provides a summary of whether the data is missing or not. It also provides the amount of missings in each columns.\n\n# Visualise missing data pattern\nvisdat::vis_miss(plant_data)\n\nOption: Removing columns with missing values\n\n# Removes all rows with NA in the dataset\nplant_data_clean &lt;- plant_data %&gt;% \n  drop_na()\n\n# not recommended if NA is scattered across columns (e.g. leaf1area) because some important variables might be dropped out.\n\n# Drop rows where NA is present in the elevation column only.\nplant_data_clean &lt;- plant_data %&gt;% \n  drop_na(elevation)\n\n# Run the vis_miss again on the clean data to check if the NA rows are removed. \nvisdat::vis_miss(plant_data_clean)\n\n\nExercise (5 min)\nüß™ Identify and handle missing values in the fake_data dataset\n\nWhich column contains missing variables?\nHow many missing values are there?\nRemove rows with missing values and create a new object called fake_data_clean.\nVisualise the new fake_data_clean with visdat::vis_dat(fake_data_clean).\n\n\n\nShow answer\n\n\n# Check for missing values in each column\ncolSums(is.na(fake_data)) # species_4, # 5 missing values\n\n# Visualise missing data pattern\nvisdat::vis_dat(fake_data) \n\n# Drop rows where NA is present in the elevation column only.\nfake_data_clean &lt;- fake_data %&gt;% \n  drop_na(species_4)\n\n# Run the vis_miss again on the clean data to check if the NA rows are removed. \nvisdat::vis_miss(fake_data_clean)",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Quality Check</span>"
    ]
  },
  {
    "objectID": "week1_getting_started.html#check-for-duplicates",
    "href": "week1_getting_started.html#check-for-duplicates",
    "title": "1¬† Quality Check",
    "section": "1.3. Check for duplicates",
    "text": "1.3. Check for duplicates\n\nThere are two common duplicate types that can happen in your dataset.\n\nRow duplicates: where the values and names of two rows are identical 99% of the time, this is user error (e.g.¬†copy pasted). These must be identified and removed.\n\n\nsum(duplicated(plant_data))  # Returns the number of duplicate rows\n\n# View the duplicate rows themselves\nplant_data[duplicated(plant_data), ]\n\nIn the plant_data, there are no row duplicates.\n\nValue duplicates: 90% of value duplicates in a column might be biologically realistic responses and is likely due to rounding of values during recording (e.g.¬†values 2.5 and 2.5 might look duplicated due to user rounding. Actual values are 2.47 and 2.49). This is usually okay and can be kept in the dataset. User error occurs if there is a pattern in duplicates (e.g.¬†2,2,6,1,2,2,9,4,2,2,5,9). This can occur due to copy and pasting in the following row by accident.\n\nYou the researcher must decide whether these duplicate responses are real or due to user error.\n\nsum(duplicated(plant_data$leaf_dry_mass))  # Returns the number of duplicate rows\n\nplant_data[duplicated(plant_data$leaf_dry_mass) | duplicated(plant_data$leaf_dry_mass, fromLast = TRUE), ] # Returns all occurrences of duplicated numbers \n\nIn the leaf_dry_mass column, there are 51 value duplicates.\n\nExercise (3 min)\nüß™ Identify the duplicate row in the fake_data dataset\n\nWhere is the duplicate row?\nRemove the duplicate row. Either manually in Excel (easy), or an R function (challenge).\nAre there other duplicate values and do you know if they are ecologically relevant duplicates (similar response) or user error?\n\n\n\nShow answer\n\n\nsum(duplicated(fake_data)) # 1 duplicate found\n\n# View the duplicate rows themselves\nfake_data[duplicated(fake_data), ] #1 duplicate row in row 31\n\nsum(duplicated(fake_data$species_1))  # species_1 = 7\nsum(duplicated(fake_data$species_2))  # species_2 = 6\nsum(duplicated(fake_data$species_3))  # species_3 = 1\nsum(duplicated(fake_data$species_4))  # species_4 = 15",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Quality Check</span>"
    ]
  },
  {
    "objectID": "week2_tidy_data.html",
    "href": "week2_tidy_data.html",
    "title": "2¬† Tidy Data",
    "section": "",
    "text": "Background reading\nWeek 2 - Tidying data and clean code\nIn this workshop, you will learn how transform raw data to something usable for analysis, and get familiar with clean code.\nSometimes, it is rare that you get the data in exactly the right form you need from the raw data you import into R. Often you‚Äôll need to create some new variables or summaries, or maybe you just want to rename the variables or reorder the observations in order to make the data a little easier to work with. In this workshop you are going to learn the five key dplyr functions (part of tidyverse) that allow you to solve the vast majority of your data manipulation challenges:\nThese can all be used in conjunction with group_by() which changes the scope of each function from operating on the entire dataset to operating on it group-by-group. These six functions provide the verbs for a language of data manipulation.\nAll verbs work similarly:\nTogether these properties make it easy to chain together multiple simple steps to achieve a complex result. Let‚Äôs dive in and see how these verbs work.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "week2_tidy_data.html#background-reading",
    "href": "week2_tidy_data.html#background-reading",
    "title": "2¬† Tidy Data",
    "section": "",
    "text": "Pick observations by their values (filter()). e.g.¬†keeping rows with values higher than 100.\nReorder the rows (arrange()). e.g.¬†rearrange a numeric column by the largest values.\nPick variables by their names (select()). e.g.¬†keeping selected columns only.\nCreate new variables with functions of existing variables (mutate()). e.g.¬†the sum of values from two numeric columns.\nCollapse many values down to a single summary (summarise()). e.g.¬†getting the overall mean of the dataset.\n\n\n\n\nThe first argument is a data frame.\nThe subsequent arguments describe what to do with the data frame, using the variable names (without quotes).\nThe result is a new data frame.\n\n\n\nStudy context\nIn order to know whether our imported data requires transformation before analysis, we need to know what we are analysing. Let‚Äôs stick with a familiar dataset, the plant_calcium.csv from this study.\nThe plant_calcium.csv dataset contains observations on sugar maple seedlings in untreated and calcium-treated watersheds at Hubbard Brook Experimental Forest in New Hampshire, USA.\nGrowth of sugar maples (Acer saccharum), known for their maple syrup and iconic leaf shape, can be stunted due to soil acidification from prolonged acid rain, which leaches calcium - a nutrient important for plant growth - from soils and stresses maple seedlings.\nTo investigate the impact of soil calcium supplementation on sugar maple seedling growth, researchers at the Hubbard Brook Long Term Ecological Research (LTER) site recorded ‚Äúgeneral sugar maple germinant health by height, leaf area, biomass, and chlorophyll content‚Äù (Juice et al., 2006) for seedlings in untreated and previously calcium-treated watersheds (Peters et al., 2004). By comparing seedling growth in calcium-treated (W1) versus untreated (Reference) watersheds, calcium impacts on sugar maple seedling growth can be explored.\n\n\n\nSugar maple seedling laboratory photos. Photo credit: Tom Siccama.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "week2_tidy_data.html#initial-check",
    "href": "week2_tidy_data.html#initial-check",
    "title": "2¬† Quality Check",
    "section": "1.1. Initial check",
    "text": "1.1. Initial check\nThis helps you spot if variables are coded incorrectly (e.g., numbers as characters or vice versa), which could be due to the researcher error before inporting the data. Some example mistakes include:\n\nIncorrect data types: Numeric columns may be read as character (text) if there are unexpected symbols, missing values coded inconsistently, or formatting issues (e.g. a column meant to be numbers may contain text or empty strings, causing R to treat it as character instead of numeric).\nInconsistent coding: Categorical variables may be inconsistently labeled (e.g., ‚ÄúMale‚Äù, ‚Äúmale‚Äù, ‚ÄúM‚Äù), leading to multiple categories that should be merged.\nHeader issues: Sometimes, the header row is missing or duplicated, or extra non-data rows are included at the top or bottom of the file.\nEncoding problems: Special characters (e.g., accents, non-English letters) may not display correctly if the file encoding is not specified or mismatched.\nTrailing spaces: Spaces at the start or end of strings can create unexpected categories or prevent proper matching.\n\n\nFor small datasets (like in this workshop) it is easier to make fixes directly from Excel and import the new .csv file. As before, we will check the data here first with the str() function.\n\n# Check the structure of your data\nstr(plant_data)\n\n# Get a summary of all variables\nsummary(plant_data)\n\n\nExercise (5 min)\nüß™ Find the incorrect class in this dataset\nI have created an example dataset called fake_data with three things wrong with it. Find the the three issues in this dataset.\n\nfake_data &lt;- data.frame(\n  site = c(\"A\", \"B\", \"C\", \"A\", \"B\", \"A\", \"B\", \"C\", \"a\", \"B\", \"A\", \"B\", \"A\", \"B\", \"C\", \"C\", \"B\", \"A\", \"A\", \"C\", \"B\", \"C\", \"C\", \"A\", \"A\", \"A\", \"A\", \"B\", \"C\", \"C\", \"C\"),\n  species_1 = c(7, 25, 30, 50, 38, 13, 5, 31, 34, 22, 41, 25, 22, 37, 39, 13, 45, 10, 40, 7, 28, 36, 19, 2, 12, 5, 16, 38, 27, 44, 44),\n  species_2 = c(9.3, 8.6, 0.8 , 7.5, 9 , 9.1, 4.3, 8.6, 0.1, 7.8, 1.2, 6.2, 0.4, 0.4, 7.4, 1.9, 8.3, 7.8, 2.9, 9.3, 9.1, 2.3, 6.3, 8.5, 6.5, 6.6, \"1,7\", 1.8, 8.4, 4.7, 4.7),\n  species_3 = c(190, 160, 70, 430, 310, 99, 530, 420, 371, 357, 198, 171, 463, 124, 254, 484, 435, 409, 122, 305, 410, 162, 473, 200, 401, 273, 421, 419, 293, 487, 487),\n  species_4 = c(2, 0, 4, 8, 6.7, 3, 7.2, 4.3, 2, NA, 10, 10, 6, 4, 8, 1.7, 1, 0, NA, 4, 6, 8.2, 10, 6, 2, 5, 10, 7, NA, NA, NA)\n)\n\nWhat are the identified issues?",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "week2_tidy_data.html#identify-missing-values",
    "href": "week2_tidy_data.html#identify-missing-values",
    "title": "2¬† Quality Check",
    "section": "1.2. Identify missing values",
    "text": "1.2. Identify missing values\nMissing data may appear as blanks, ‚ÄúNA‚Äù, ‚Äú.‚Äù, or other placeholders. If not handled properly, this can cause errors or misinterpretation during analysis.\n\n\n# Check for missing values in each column\ncolSums(is.na(plant_data))\n\n# Visualise missing data pattern\nvisdat::vis_dat(plant_data)\n\nvis_dat() visualises the whole dataframe at once, and provides information about the class of the data input into R, as well as whether the data is missing or not.\nThe function vis_miss() provides a summary of whether the data is missing or not. It also provides the amount of missings in each columns.\n\n# Visualise missing data pattern\nvisdat::vis_miss(plant_data)\n\nOption: Removing columns with missing values\n\n# Removes all rows with NA in the dataset\nplant_data_clean &lt;- plant_data %&gt;% \n  drop_na()\n\n# not recommended if NA is scattered across columns (e.g. leaf1area) because some important variables might be dropped out.\n\n# Drop rows where NA is present in the elevation column only.\nplant_data_clean &lt;- plant_data %&gt;% \n  drop_na(elevation)\n\n# Run the vis_miss again on the clean data to check if the NA rows are removed. \nvisdat::vis_miss(plant_data_clean)\n\n\nExercise (5 min)\nüß™ Identify and handle missing values in the fake_data dataset\n\nWhich column contains missing variables?\nHow many missing values are there?\nRemove rows with missing values and create a new object called fake_data_clean.\nVisualise the new fake_data_clean with visdat::vis_dat(fake_data_clean).",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "week2_tidy_data.html#check-for-duplicates",
    "href": "week2_tidy_data.html#check-for-duplicates",
    "title": "2¬† Quality Check",
    "section": "1.3. Check for duplicates",
    "text": "1.3. Check for duplicates\n\nThere are two common duplicate types that can happen in your dataset.\n\nRow duplicates: where the values and names of two rows are identical 99% of the time, this is user error (e.g.¬†copy pasted). These must be identified and removed.\n\n\nsum(duplicated(plant_data))  # Returns the number of duplicate rows\n\n# View the duplicate rows themselves\nplant_data[duplicated(plant_data), ]\n\nIn the plant_data, there are no row duplicates.\n\nValue duplicates: 90% of value duplicates in a column might be biologically realistic responses and is likely due to rounding of values during recording (e.g.¬†values 2.5 and 2.5 might look duplicated due to user rounding. Actual values are 2.47 and 2.49). This is usually okay and can be kept in the dataset. User error occurs if there is a pattern in duplicates (e.g.¬†2,2,6,1,2,2,9,4,2,2,5,9). This can occur due to copy and pasting in the following row by accident.\n\nYou the researcher must decide whether these duplicate responses are real or due to user error.\n\nsum(duplicated(plant_data$leaf_dry_mass))  # Returns the number of duplicate rows\n\nIn the leaf_dry_mass column, there are 51 value duplicates.\n\nExercise (3 min)\nüß™ Identify the duplicate row in the fake_data dataset\n\nWhere is the duplicate row?\nRemove the duplicate row. Either manually in Excel (easy), or an R function (challenge).\nAre there other duplicate values and do you know if they are ecologically relevant duplicates (similar response) or user error?",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "week2_tidy_data.html#check-for-outliers",
    "href": "week2_tidy_data.html#check-for-outliers",
    "title": "2¬† Quality Check",
    "section": "2.1 Check for outliers",
    "text": "2.1 Check for outliers\nOutliers aren‚Äôt inherently ‚Äúbad,‚Äù but they can cause problems depending on the context and goals of your analysis. They can:\n\nSkew statistical results: A single extreme value can pull the mean far from the center, increase variability making data seem more spread than it really us, and can distrort the slope and intercept of linear models, leading to misleading predictions.\nCreate obscure patterns: Hide clusters or trends that would otherwise be visible.\nIndicate user error: Data entry mistakes (e.g., typing 1000 instead of 100), measurement errors (e.g., faulty sensors), incorrect data merging (e.g., mixing units like metres and feet).\n\nThere are different ways we can check for outliers. See a more comprehensive page for more options. Here, we will create two common plots to check for outliers, and run some simple methods for detecting and removing outliers.\n\nBoxplot\nWe can use the ggplot() function from the ggplot package to create a boxplot via geom_boxplot() showing the distribution of the lead2area data.\n\n# Boxplot of leaf2area to detect potential outliers\nggplot(plant_data, aes(y = leaf2area)) +\n  geom_boxplot(fill = \"lightgrey\") +\n  theme_bw()\n\nWarning: Removed 117 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\nHistogram\nAnother way we can visualise outliers is with histograms via geom_histogram() or density plots via geom_density()\n\n# Boxplot of height to detect potential outliers\nggplot(plant_data, aes(x = leaf2area)) +\n  geom_histogram() +\n  theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\nWarning: Removed 117 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nHere you can see one clear outlier with a value above 100. All other values are below 25. You can either manually remove the outlier on Excel, or with the following code below.\nOption: Remove outliers using the interquartile range (IQR) rule.\n\n# View potential outliers using IQR rule\nQ1 &lt;- quantile(plant_data$leaf2area, 0.25, na.rm = TRUE)\nQ3 &lt;- quantile(plant_data$leaf2area, 0.75, na.rm = TRUE)\nIQR &lt;- Q3 - Q1\n\n# Filter rows with height outside 1.5 * IQR range\noutliers &lt;- filter(plant, height &lt; (Q1 - 1.5 * IQR) | height &gt; (Q3 + 1.5 * IQR))\noutliers\n\n\n\n\n\n\n\nNote\n\n\n\nDon‚Äôt just blindly remove outliers!\n\n\nMake sure the numbers are biologically realistic based on intuition and comparing with the literature. If the values are unrealistic, it is a safe assumption to remove them.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "week2_tidy_data.html#is-the-data-normally-distributed",
    "href": "week2_tidy_data.html#is-the-data-normally-distributed",
    "title": "2¬† Quality Check",
    "section": "2.2 Is the data normally distributed",
    "text": "2.2 Is the data normally distributed\nThis step is required to check if the data is suitable for certain types of analysis. Sometimes, transformation is required for certain data to meet the assumptions of certain statistical methods and improve the interpretability and performance of models. Many statistical techniques‚Äîlike linear regression, ANOVA, and t-tests‚Äîassume that the data is normally distributed, has constant variance, and is linearly related (from your MAS183 - Statistical Data Analysis and MAS224 - Biostatistical Methods units). If the data is skewed, has outliers, or shows non-constant variance (heteroscedasticity), these assumptions are violated, which can lead to biased estimates, incorrect conclusions, or poor model fit.\nBy applying transformations such as logarithmic, square root, or Box-Cox, we can:\n\nReduce skewness and make the distribution more symmetrical.\nStabilise variance across levels of an independent variable.\nImprove linearity between variables.\nMinimise the influence of outliers.\n\nLet‚Äôs check the distribution of leaf2area after removing that one outlier.\n\n # remove values above 100\nplant_data_clean &lt;- plant_data %&gt;%\n  filter(leaf2area &lt; 100)\n\nggplot(plant_data_clean, aes(x = leaf2area)) +\n  geom_histogram() +\n  theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\nLooks normally distributed to me! We will learn data transformation in next weeks workshop.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "week2_tidy_data.html#for-the-adventurous-folks",
    "href": "week2_tidy_data.html#for-the-adventurous-folks",
    "title": "2¬† Tidy Data",
    "section": "For the adventurous folks",
    "text": "For the adventurous folks\nYou‚Äôve learned how to use the core dplyr verbs ‚Äî now let‚Äôs practice building more powerful and efficient workflows by combining multiple functions in clean, compact code.\n1. Take your cleaned dataset plant_task_data_clean and write one pipeline that:\n\nFilters only W1 watershed and year 2004.\nCreates a new variable total_mass_g from stem_dry_mass + leaf_dry_mass, multiplied by 1000\nSelects only the new variable and stem_length\nFilters any rows with stem_length greater than 200 mm.\nArranges the result by total_mass_g in descending order.\n\nUse %&gt;% or |&gt; and avoid creating intermediate objects.\n2. Using group_by() and summarise() in a single chain:\n\nGroup the data by watershed and year.\nCalculate the mean, coefficient of variation, and count of total_mass_g.\nArrange by year in ascending order.\n\n3. Write a function that:\n\nTakes a numeric column as input.\nFilters out outliers using the 1.5 √ó IQR rule.\nReturns the filtered data.\n\nTry applying it to leaf_dry_mass and stem_length.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "week2_tidy_data.html#why-is-my-code-not-working",
    "href": "week2_tidy_data.html#why-is-my-code-not-working",
    "title": "2¬† Quality Check",
    "section": "Why is my code not working!",
    "text": "Why is my code not working!\nComputers are only as smart as the humans that use them.\nIf your code is not working there is most likely a spelling mistake or a typographic error. These human errors are easy to miss but equally easy to fix!\nIf your code is not working take a moment to breathe. Then check for common, minor errors such as:\n\nSpelling mistakes\nWrong dataset name\nWrong variable (column) name\nMissing or wrong quotation mark\nMissing bracket\nInconsistent cases (e.g.¬†Uppercase)\nMissed a step\nInvalid syntax (e.g.¬†spaces)\nDuplicates of the same function with multiple errors ‚Äì keep your scripts tidy!\nThese reading/typing mistakes are the majority of encountered errors. They are not a big deal and are easily corrected.\n\nYou can and should easily fix the above mistakes yourself! We want you to know how to work problems out independently.\n\nExercise\nüß™ Why does this code not work?\n\nmy_variable &lt;- 10\nmy_varƒ±able\n#&gt; Error: object 'my_varƒ±able' not found\n\nLook carefully! (This may seem like an exercise in pointlessness, but training your brain to notice even the tiniest difference will pay off when programming.)",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "week2_tidy_data.html#additional-resource",
    "href": "week2_tidy_data.html#additional-resource",
    "title": "2¬† Quality Check",
    "section": "Additional resource",
    "text": "Additional resource\n\nHow to rename factor levels in R.\nCrashCourse Statistics video on Plots, Outliers, and Justin Timberlake, with examples of went to remove outliers.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "week2_tidy_data.html#exercise-10-min",
    "href": "week2_tidy_data.html#exercise-10-min",
    "title": "2¬† Tidy Data",
    "section": "Exercise (10 min)",
    "text": "Exercise (10 min)\nüß™ Use the previous functions to complete the following task for the fake_data below\n\nfake_data &lt;- data.frame(\n  site = c(\"A\", \"B\", \"C\", \"A\", \"B\", \"A\", \"B\", \"C\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"C\", \"C\", \"B\", \"A\", \"b\", \"C\", \"B\", \"C\", \"C\", \"A\", \"A\", \"A\", \"A\", \"B\", \"C\", \"C\", \"C\"),\n  species_1 = as.integer(c(7, 25, 30, 50, 38, 13, 5, 31, 34, 22, 41, 25, 22, 37, 39, 13, 45, 10, 40, 7, 28, 36, 19, 2, 12, 5, 16, 38, 27, 44, 44)),\n  species_2 = as.integer(c(9, 8, 1 , 7, 9 , 9, 4, 8, 1, 7, 1, 6, 4, 4, 7, 2, 8, 7, 2, 9, 9, 2, 6, 8, 6, 6, 17, 1, 8, 4, 4)),\n  species_3 = as.integer(c(190, 160, 70, 430, 310, 99, 530, 420, 371, 357, 198, 171, 463, 124, 254, 484, 435, 409, 122, 305, 410, 162, 473, 200, 401, 273, 421, 419, 293, 487, 487)),\n  species_4 = as.integer(c(2, 0, 4, 8, 6, 3, 7, 4, 2, NA, 35, 10, 6, 4, 8, 1, 1, 0, NA, 4, 6, 8, 10, 6, 2, 5, 10, 7, NA, NA, NA))\n  )\n\n\nConduct the usual quality check on this dataset (slightly different mistakes). To save time, there is an inconsistent coding in one of the categorical variables, and an extreme value in one of the numeric variables to remove.\nCreate a new variable column called total_sp that combines all the species.\nSelect only the site and total_sp variable into a new dataframe.\nCalculate the mean, minimum, maximum, and count of total_sp by site.\n\nQuestion:\n\nWhat is the mean total abundance in site B?\nWhat is the problem with using mean here? Is there a better way to calculate central tendancy for dount data? (hint: think of decimals in this context).\nWhich site has the highest average species abundance?\nWhich site has the most replicate samples?\n\n\n\nShow answer\n\n\nstr(fake_data)\n\nfake_data_clean &lt;- fake_data %&gt;%\n  dplyr::mutate(site = recode(site, \"b\" = \"B\"),\n                site = as.factor(site)) %&gt;%\n  dplyr::filter(species_4 &lt; 30) %&gt;%\n  dplyr::mutate(total_sp = species_1 + species_2 + species_3 + species_4) %&gt;%\n  dplyr::select(site, total_sp)\n\nfake_data_clean %&gt;%\n  dplyr::group_by(site) %&gt;%\n  dplyr::summarise(mean = mean(total_sp),\n                   min  = min(total_sp),\n                   max  = max(total_sp),\n                   n = n())\n\n# 1. 361.25 \n# 2. Mode more suitable for integers\n# 3. Site B\n# 4. site A",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "week2_tidy_data.html#exercise-15-min",
    "href": "week2_tidy_data.html#exercise-15-min",
    "title": "2¬† Tidy Data",
    "section": "Exercise (15 min)",
    "text": "Exercise (15 min)\nüß™ Use the previous functions to complete the following task for the fake_data below\n\nfake_data &lt;- data.frame(\n  site = c(\"A\", \"B\", \"C\", \"A\", \"B\", \"A\", \"B\", \"C\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"C\", \"C\", \"B\", \"A\", \"b\", \"C\", \"B\", \"C\", \"C\", \"A\", \"A\", \"A\", \"A\", \"B\", \"C\", \"C\", \"C\"),\n  species_1 = as.integer(c(7, 25, 30, 50, 38, 13, 5, 31, 34, 22, 41, 25, 22, 37, 39, 13, 45, 10, 40, 7, 28, 36, 19, 2, 12, 5, 16, 38, 27, 44, 44)),\n  species_2 = as.integer(c(9, 8, 1 , 7, 9 , 9, 4, 8, 1, 7, 1, 6, 4, 4, 7, 2, 8, 7, 2, 9, 9, 2, 6, 8, 6, 6, 17, 1, 8, 4, 4)),\n  species_3 = as.integer(c(190, 160, 70, 430, 310, 99, 530, 420, 371, 357, 198, 171, 463, 124, 254, 484, 435, 409, 122, 305, 410, 162, 473, 200, 401, 273, 421, 419, 293, 487, 487)),\n  species_4 = as.integer(c(2, 0, 4, 8, 6, 3, 7, 4, 2, NA, 35, 10, 6, 4, 8, 1, 1, 0, NA, 4, 6, 8, 10, 6, 2, 5, 10, 7, NA, NA, NA))\n  )\n\n\nConduct the usual quality check on this dataset (slightly different mistakes).\nCreate a new variable column called total_sp that combines all the species.\nSelect only the site and total_sp variable into a new dataframe.\nCalculate the mean, minimum, maximum, and count of total_sp by site.\n\nQuestion:\n\nWhat is the mean total abundance in site B?\nWhat is the problem with using mean here? Is there a better way to calculate central tendancy for dount data? (hint: think of decimals in this context).\nWhich site has the highest average species abundance?\nWhich site has the most replicate samples?\n\n\n\nShow answer\n\n\nstr(fake_data)\n\n# there is an inconsistent coding in one of the categorical variables, and an extreme value in one of the numeric variables to remove.\n\nfake_data_clean &lt;- fake_data %&gt;%\n  dplyr::mutate(site = recode(site, \"b\" = \"B\"),\n                site = as.factor(site)) %&gt;%\n  dplyr::filter(species_4 &lt; 30) %&gt;%\n  dplyr::mutate(total_sp = species_1 + species_2 + species_3 + species_4) %&gt;%\n  dplyr::select(site, total_sp)\n\nfake_data_clean %&gt;%\n  dplyr::group_by(site) %&gt;%\n  dplyr::summarise(mean = mean(total_sp),\n                   min  = min(total_sp),\n                   max  = max(total_sp),\n                   n = n())\n\n# 1. 361.25 \n# 2. Mode more suitable for integers\n# 3. Site B\n# 4. site A",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "week1_getting_started.html#check-for-outliers",
    "href": "week1_getting_started.html#check-for-outliers",
    "title": "1¬† Quality Check",
    "section": "2.1 Check for outliers",
    "text": "2.1 Check for outliers\nOutliers aren‚Äôt inherently ‚Äúbad,‚Äù but they can cause problems depending on the context and goals of your analysis. They can:\n\nSkew statistical results: A single extreme value can pull the mean far from the center, increase variability making data seem more spread than it really us, and can distrort the slope and intercept of linear models, leading to misleading predictions.\nCreate obscure patterns: Hide clusters or trends that would otherwise be visible.\nIndicate user error: Data entry mistakes (e.g., typing 1000 instead of 100), measurement errors (e.g., faulty sensors), incorrect data merging (e.g., mixing units like metres and feet).\n\nThere are different ways we can check for outliers. See a more comprehensive page for more options. Here, we will create two common plots to check for outliers, and run some simple methods for detecting and removing outliers.\n\nBoxplot\nWe can use the ggplot() function from the ggplot package to create a boxplot via geom_boxplot() showing the distribution of the lead2area data.\n\n# Boxplot of leaf2area to detect potential outliers\nggplot(plant_data, aes(y = leaf2area)) +\n  geom_boxplot(fill = \"lightgrey\") +\n  theme_bw()\n\nWarning: Removed 117 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\nHistogram\nAnother way we can visualise outliers is with histograms via geom_histogram() or density plots via geom_density()\n\n# Boxplot of height to detect potential outliers\nggplot(plant_data, aes(x = leaf2area)) +\n  geom_histogram() +\n  theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\nWarning: Removed 117 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nHere you can see one clear outlier with a value above 100. All other values are below 25. You can either manually remove the outlier on Excel, or with the following code below.\nOption: Remove outliers using the interquartile range (IQR) rule.\n\n# View potential outliers using IQR rule\nQ1 &lt;- quantile(plant_data$leaf2area, 0.25, na.rm = TRUE)\nQ3 &lt;- quantile(plant_data$leaf2area, 0.75, na.rm = TRUE)\nIQR &lt;- Q3 - Q1\n\n# Filter rows with height outside 1.5 * IQR range\noutliers &lt;- filter(plant, height &lt; (Q1 - 1.5 * IQR) | height &gt; (Q3 + 1.5 * IQR))\noutliers\n\nMake sure the numbers are biologically realistic based on intuition and comparing with the literature. If the values are unrealistic, it is a safe assumption to remove them.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Quality Check</span>"
    ]
  },
  {
    "objectID": "week1_getting_started.html#is-the-data-normally-distributed",
    "href": "week1_getting_started.html#is-the-data-normally-distributed",
    "title": "1¬† Quality Check",
    "section": "2.2 Is the data normally distributed",
    "text": "2.2 Is the data normally distributed\nThis step is required to check if the data is suitable for certain types of analysis. Sometimes, transformation is required for certain data to meet the assumptions of certain statistical methods and improve the interpretability and performance of models. Many statistical techniques‚Äîlike linear regression, ANOVA, and t-tests‚Äîassume that the data is normally distributed, has constant variance, and is linearly related (from your MAS183 - Statistical Data Analysis and MAS224 - Biostatistical Methods units). If the data is skewed, has outliers, or shows non-constant variance (heteroscedasticity), these assumptions are violated, which can lead to biased estimates, incorrect conclusions, or poor model fit.\nBy applying transformations such as logarithmic, square root, or Box-Cox, we can:\n\nReduce skewness and make the distribution more symmetrical.\nStabilise variance across levels of an independent variable.\nImprove linearity between variables.\nMinimise the influence of outliers.\n\nLet‚Äôs check the distribution of leaf2area after removing that one outlier.\n\n# remove values above 100\nplant_data_clean &lt;- plant_data %&gt;%\n  filter(leaf2area &lt; 100)\n\nggplot(plant_data_clean, aes(x = leaf2area)) +\n  geom_histogram() +\n  theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\nLooks normally distributed to me! We will learn data transformation in next weeks workshop.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Quality Check</span>"
    ]
  },
  {
    "objectID": "week1_getting_started.html#for-the-adventurous-folks-1",
    "href": "week1_getting_started.html#for-the-adventurous-folks-1",
    "title": "1¬† Quality Check",
    "section": "For the adventurous folks",
    "text": "For the adventurous folks\nHere we will start thinking about writing code efficiently. For example, you would have noticed by now that whatever raw data we load in R, any categorical columns are automatically listed as characters. So you can write a code that both loads the data and change the characters to factors in the same line of code (e.g.¬†using the dplyr::mutate() function). Load the plant_calcium_task.csv and change categorical variables from character and factor.\nRepeat the same assessment as above, but instead of correcting the errors by editing them on Excel, do all the corrections within the R environment. Visualise the clean version of the the data with a density plot but separate the density by the treatment groups (reference and W1 variables).",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Quality Check</span>"
    ]
  },
  {
    "objectID": "week1_getting_started.html#why-is-my-code-not-working",
    "href": "week1_getting_started.html#why-is-my-code-not-working",
    "title": "1¬† Quality Check",
    "section": "Why is my code not working!",
    "text": "Why is my code not working!\nComputers are only as smart as the humans that use them.\nIf your code is not working there is most likely a spelling mistake or a typographic error. These human errors are easy to miss but equally easy to fix!\nIf your code is not working take a moment to breathe. Then check for common, minor errors such as:\n\nSpelling mistakes\nWrong dataset name\nWrong variable (column) name\nMissing or wrong quotation mark\nMissing bracket\nInconsistent cases (e.g.¬†Uppercase)\nMissed a step\nInvalid syntax (e.g.¬†spaces)\nDuplicates of the same function with multiple errors ‚Äì keep your scripts tidy!\nThese reading/typing mistakes are the majority of encountered errors. They are not a big deal and are easily corrected.\n\nYou can and should easily fix the above mistakes yourself! We want you to know how to work problems out independently.\n\nExercise\nüß™ Why does this code not work?\n\nmy_variable &lt;- 10\nmy_varƒ±able\n#&gt; Error: object 'my_varƒ±able' not found\n\nLook carefully! (This may seem like an exercise in pointlessness, but training your brain to notice even the tiniest difference will pay off when programming.)",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Quality Check</span>"
    ]
  },
  {
    "objectID": "week1_getting_started.html#additional-resource",
    "href": "week1_getting_started.html#additional-resource",
    "title": "1¬† Quality Check",
    "section": "Additional resource",
    "text": "Additional resource\n\nLoading other file types (if not .csv).\nHow to rename factor levels in R.\nCrashCourse Statistics video on Plots, Outliers, and Justin Timberlake, with examples of went to remove outliers.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Quality Check</span>"
    ]
  },
  {
    "objectID": "week3_diversity_index.html#calculations-in-r",
    "href": "week3_diversity_index.html#calculations-in-r",
    "title": "3¬† Diversity Indices",
    "section": "3.1 Calculations in R",
    "text": "3.1 Calculations in R\n\nAbsolute abundanceRelative abundance\n\n\nEquation:\n\\[\nN_i = \\sum_{j=1}^{k} n_{ij}\n\\] where \\(n_{ij}\\) - indidivuals of species \\(i\\) in sample \\(j\\).\nTo calculate absolute abundance of one species in R, here is the following code.\n\n# for one species\nsum(species_counts) # Total abundance for all or individual species\n\nYou may want to calculate abundance for each species, by site.\n\n\nEquation:\n\\[\np_i = \\frac{n_i}{N}\n\\]\nwhere \\(p_i\\) is the relative abundance of species \\(i\\), \\(n_i\\) is the abundance of species \\(i\\), \\(N\\) is the total abundance across all species.\nTo calculate relative abundance in R, here is the following code.\n\nrelative_abundance &lt;- species_counts / sum(species_counts)",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Diversity Indices</span>"
    ]
  },
  {
    "objectID": "week3_diversity_index.html#calculations-of-abundance",
    "href": "week3_diversity_index.html#calculations-of-abundance",
    "title": "3¬† Diversity Indices",
    "section": "Calculations of abundance",
    "text": "Calculations of abundance\nWe will start with this fake ecological survey data. Also, to calculate Shannon and Simpson‚Äôs diversity, we will need the the vegan package.\n\n#install.packages('vegan')\n\n# Load library\nlibrary(tidyverse)\nlibrary(vegan)\n\n# Create fake survey data\nfake_data &lt;- data.frame(\n  site = c(\"A\",\"A\",\"A\",\"A\",\"A\",\n           \"B\",\"B\",\"B\",\"b\",\"B\",\n           \"C\",\"C\",\"C\",\"C\",\"C\"),\n  species_1 = c(57,73,20,75,83,\n                41,6,61,45,2,\n                0,93,0,0,86),\n  species_2 = c(31,12,4,41,4,\n                49,2,7,7,9,\n                54,64,35,38,53),\n  species_3 = c(0,38,3,0,39,\n                0,29,24,15,23,\n                10,0,1,5,2),\n  species_4 = c(11,28,19,0,1,\n                34,7,0,1,0,\n                1,0,12,45,5),\n  species_5 = c(0,0,0,0,0,\n                4,5,0,0,0,\n                3,1,0,1,2)\n)\n\n# Clean data identified with incosistencies in format\nfake_data_clean &lt;- fake_data %&gt;%\n  dplyr::mutate(site = recode(site, \"b\" = \"B\"),\n                site = as.factor(site))\n\n\nAbsolute abundanceRelative abundanceDensity\n\n\nEquation:\n\\[\nN_i = \\sum_{j=1}^{k} n_{ij}\n\\] where \\(n_{ij}\\) - individuals of species \\(i\\) in sample \\(j\\). Essentially its just the number of individuals of that species counted.\nTo calculate absolute abundance of one species in R, here is the following code.\n\n# for one species\nsum(fake_data_clean$species_1) # Total abundance for all or individual species across all sites\n\nYou can see that the total number of species_1 in the survey is 642.\nYou may want to calculate abundance for each species, by site.\n\nfake_data_long &lt;- fake_data_clean %&gt;% \n  tidyr::pivot_longer(!site, names_to = \"species\", values_to = \"abundance\") # convert wide format to long format (easier to summaries groups)\n\nabundance_sum &lt;- fake_data_long %&gt;%\n  group_by(site, species) %&gt;%\n  summarise(total = sum(abundance))\n\n# Visulise abundance\nabundance_sum %&gt;%\n  ggplot() +\n  geom_bar(aes(x = species, y = total), stat = \"identity\") +\n  labs(x = \"Species\", y = \"Total abundance\") +\n  theme_bw() +\n  coord_flip()\n\n\n\n\n\n\n\n\nYou can see that the total number of species_1 is 308 in site A, 155 in site B, and 179 in site C.\n\n\nEquation:\n\\[\np_i = \\frac{N_i}{\\sum_{i=1}^{S} N_i} = \\frac{N_i}{N}\n\\]\nwhere \\(p_i\\) is the relative abundance of species \\(i\\), \\(N_i\\) is the abundance of species \\(i\\), \\(\\sum_{i=1}^{S} N_i = N\\) is the total abundance across all species in the community, \\(S\\) is the total number of species.\nFor example, if 10 of the 50 individuals in a quadrat are species A: \\(p_i\\) = 10/50 = 0.20, meaning species A makes up 20% of the community.\nTo calculate relative abundance of each species in R, here is the following code.\n\n# Option 1 - using wide format data\nfake_data_clean %&gt;%\n  select(starts_with(\"species_\")) %&gt;% # select all columns with \"species_\"\n  summarise(across(everything(), sum)) %&gt;%      # For each column (using across), calculate the sum of all values (everything). i.e total abundance per species\n  mutate(total = sum(across(everything()))) %&gt;% # total abundance of all species\n  mutate(across(starts_with(\"species_\"), ~ .x / total)) # For each species column, take its values (.x) and divide by total.\n\n  species_1 species_2 species_3 species_4  species_5 total\n1 0.4517945 0.2885292 0.1330049 0.1154117 0.01125968  1421\n\n# Option 2 - using long format data\nrelative_sum &lt;- fake_data_long %&gt;%\n  group_by(species) %&gt;%\n  summarise(species_sum = sum(abundance),\n         relative_abundance = species_sum / sum(fake_data_long$abundance))\n\n# Visualise relative abundance by species\nrelative_sum %&gt;%\n  ggplot() +\n  geom_bar(aes(x = species, y = relative_abundance), stat = \"identity\") +\n  labs(x = \"Species\", y = \"Relative abundance\") +\n  theme_bw() +\n  coord_flip()\n\n\n\n\n\n\n\n\nTo calculate relative abundance of each species within each site.\n\n# Option 1 - using wide format data\nfake_data_clean %&gt;%\n  group_by(site) %&gt;%\n  summarise(across(starts_with(\"species_\"), sum)) %&gt;%   # total abundance per species per site\n  mutate(total = rowSums(across(starts_with(\"species_\")))) %&gt;%  # site-level total abundance\n  mutate(across(starts_with(\"species_\"), ~ .x / total)) # divide species abundance by site total\n\n# A tibble: 3 √ó 7\n  site  species_1 species_2 species_3 species_4 species_5 total\n  &lt;fct&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 A         0.571     0.171    0.148      0.109    0        539\n2 B         0.418     0.199    0.245      0.113    0.0243   371\n3 C         0.350     0.477    0.0352     0.123    0.0137   511\n\n# Option 2 - using long format data\nrelative_site_sum &lt;- fake_data_long %&gt;%\n  group_by(site, species) %&gt;%\n  summarise(total_abundance = sum(abundance)) %&gt;%\n  mutate(site_total = sum(total_abundance),\n         rel_abundance = total_abundance / site_total)\n\n# Visualise results\nrelative_site_sum %&gt;%\n  ggplot() +\n  geom_bar(aes(x = site, y = rel_abundance, fill = species), stat = \"identity\", position = position_dodge()) +\n  labs(x = \"Sites\", y = \"Relative abundance\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nWe see that species_1 makes up about 57% of the sample for Site A, and species_4 makes up about 11% of the sample. Once we calculate relative densities for each species at each site, this eliminates differences in total density at each site because all sites then total to 1.\n\n\nEquation:\n\\[\nd_i = \\frac{N_i}{A}\n\\]\nwhere \\(d_i\\) is the density of species \\(i\\) (e.g, individuals/ha), \\(N_i\\) absolute abundance of species \\(i\\), \\(A\\) is the area sampled (e.g.¬†m2, ha, km2).\nFor example, if you counted 42 lizards in a 0.5 ha plot: \\(d_i\\) = 42/0.5 = 84 individuals/ha.\nTo calculate density for each site per sampling event (individuals/site/sampling event) in R, here is the following code.\n\n# Option 1 - wide format\nfake_data_clean %&gt;%\n  group_by(site) %&gt;%\n  summarise(round(across(starts_with(\"species\"), mean, na.rm = TRUE))) # calculates the mean count per site across the sampling period.\n\n# A tibble: 3 √ó 6\n  site  species_1 species_2 species_3 species_4 species_5\n  &lt;fct&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 A            62        18        16        12         0\n2 B            31        15        18         8         2\n3 C            36        49         4        13         1\n\n# Option 2 - long format\ndensity_sum &lt;- fake_data_long %&gt;%\n  group_by(site, species) %&gt;%\n  summarise(density = round(mean(abundance))) # calculates the mean count per site across the sampling period.\n\n# Visualise results\ndensity_sum %&gt;%\n  ggplot() +\n  geom_bar(aes(x = site, y = density, fill = species), stat = \"identity\", position = position_dodge()) +\n  labs(x = \"Sites\", y = \"Density (n/sample effort)\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nWe can see here that site A has 62 individuals per sampling effort for species_1, while site B has 31 indidvuals per sampling effort for species_1.\nIf you want density of all individuals by site, then use the following code.\n\nfake_data_long %&gt;%\n  group_by(site) %&gt;%\n  summarise(density = round(mean(abundance))) # calculates the mean count per site across the sampling period.\n\nSite A has an average of 22 individuals per sampling effort, while site B has an average of 20 individuals per sampling effort.\n\n\n\n\nExercise 1 (10 min)\nüß™ Complete the following task for the excerise_survey_data below\n\nCalculate the absolute abundance, relative abundance, and density of all species by sites\nWhat is the absolute density of species_1?\nWhat is the relative abundance of species_2 in site B?\nWhat is the mean density of species_3?\n\n\nexcerise_survey_data &lt;- data.frame(\n  site = c(\"A\",\"A\",\"A\",\"B\",\"B\",\"B\",\"C\",\"C\",\"C\"),\n  plot = c(1,2,3,1,2,3,1,2,3),\n  species_1 = c(12, 7, 15, 4, 9, 6, 20, 18, 25),\n  species_2 = c(5, 2, 1, 10, 7, 4, 3, 1, 0),\n  species_3 = c(0, 3, 2, 6, 4, 5, 0, 1, 2),\n  species_4 = c(8, 6, 3, 1, 4, 2, 10, 7, 5)\n)\n\n\n\nShow answer\n\n\nexcerise_survey_long &lt;- excerise_survey_data %&gt;%\n  tidyr::pivot_longer(!c(site, plot), names_to = \"species\", values_to = \"abundance\")# convert wide format to long format (easier to summaries groups)\n  \nexcerise_survey_sum &lt;- excerise_survey_long %&gt;%\n  group_by(site, species) %&gt;%\n  summarise(species_sum = sum(abundance), # calculates abundance\n            relative = (species_sum / sum(excerise_survey_long$abundance) * 100), # calculates relative abundance\n            density = round(mean(abundance)) # calculates density (individuals/plot)\n            ) \n\n# answer 1 \nexcerise_survey_sum %&gt;%\n  filter(species == \"species_1\") %&gt;%\n  pull(species_sum) %&gt;%\n  sum()\n\n# answer 2\nexcerise_survey_sum %&gt;%\n  filter(species == \"species_2\", site == \"B\") %&gt;%\n  select(relative) \n\n# answer 3\nexcerise_survey_sum %&gt;%\n  filter(species == \"species_3\") %&gt;%\n  pull(density) %&gt;%\n  mean()\n\n\n# 1. 116\n# 2. 9%\n# 3. 2.6",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Diversity Indices</span>"
    ]
  },
  {
    "objectID": "week3_diversity_index.html#calculations-of-species-richness",
    "href": "week3_diversity_index.html#calculations-of-species-richness",
    "title": "3¬† Diversity Indices",
    "section": "Calculations of species richness",
    "text": "Calculations of species richness\n\nSpecies richnessRarefied richness\n\n\nEquation:\n\\[\nS = number \\, of \\, species \\, observed\n\\]\nTo calculate species richness in R, here is the following code. Note, you will need to remove zeros when using length() to count number of species because length will include zero.\n\n# Option 1 - using tidyverse\nfake_data_long %&gt;%\n  filter(abundance != 0) %&gt;% # remove rows with zeros (e.g. no species 5 in site A)\n  group_by(site) %&gt;%\n  summarise(total = length(unique(species))) \n\nIn our fake dataset, we have four species in site A, and all five species in site B and C. Note, if we includes zeros in this calculation, we would get species_5 in the count which means all sites would have five species, which is incorrect.\n\n\nSpecies richness increases with sample size, and differences in richness actually may be caused by differences in sample size. To solve this problem, we may try to rarefy species richness to the same number of individuals.\nTo calculate absolute abundance of one species in R, here is the following code.\n\n# sum species counts per site\nsite_counts &lt;- fake_data_clean %&gt;%\n  group_by(site) %&gt;%\n  summarise(across(where(is.numeric), sum))\n\n\n# Rarefied richness needs a ‚Äúsample size‚Äù = smallest total abundance across sites.\nnmin &lt;- rowSums(site_counts[ , -1]) %&gt;% min()\n\nrarefied_richness_output &lt;- vegan::rarefy(site_counts[ , -1], sample = nmin)\n\n# add rarefied results to sites\nrarefied_richness &lt;- bind_cols(site = site_counts$site,\n                               rarefied_richness = rarefied_richness_output)\n\nIn this fake dataset, the results are the same as your standard species richness count. When you have more complicated and more variable datasets with lots of rare species, this calculation might be useful. Always justify which metric you plan to use in your report.\n\n\n\n\nExercise 2 (10 min)\nüß™ Complete the following task for the excerise_survey_data below\n\nCalculate the species richness by sites.\nWhat is the richness of site C?\n\n\n\nShow answer\n\n\n# Species richness\nexcerise_survey_long %&gt;%\n  filter(abundance != 0) %&gt;% # remove rows with zeros (e.g. no species 5 in site A)\n  group_by(site) %&gt;%\n  summarise(total = length(unique(species))) \n\n# 1. 4",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Diversity Indices</span>"
    ]
  },
  {
    "objectID": "week3_diversity_index.html#exercise-1-10-min",
    "href": "week3_diversity_index.html#exercise-1-10-min",
    "title": "3¬† Diversity Indices",
    "section": "Exercise 1 (10 min)",
    "text": "Exercise 1 (10 min)\nüß™ Complete the following task for the fake_data_clean below\n\nCalculate the absolute abundance, relative abundance, and density of all species by sites\nWhat is the absolute density of species_1?\nWhat is the relative abundance of species_2?\nWhat is the density of species_3?",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Diversity Indices</span>"
    ]
  },
  {
    "objectID": "week3_diversity_index.html#exercise-2-10-min",
    "href": "week3_diversity_index.html#exercise-2-10-min",
    "title": "3¬† Diversity Indices",
    "section": "Exercise 2 (10 min)",
    "text": "Exercise 2 (10 min)\nüß™ Use the previous R code and functions from the previous week to complete the following task for the fake_data_clean below",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Diversity Indices</span>"
    ]
  },
  {
    "objectID": "week3_diversity_index.html#calculations-of-diversity",
    "href": "week3_diversity_index.html#calculations-of-diversity",
    "title": "3¬† Diversity Indices",
    "section": "Calculations of diversity",
    "text": "Calculations of diversity\n\nShannon diversity indexSimpson‚Äôs diversity index\n\n\nEquation:\n\\[\nH' = - \\sum_{i=1}^{S} p_i \\,\\ln(p_i)\n\\] where \\(H'\\) is Shannon diversity index, \\(S\\) is the total number of species in the community (species richness), \\(p_i\\) is the relative abundance of species \\(i\\) (see relative abundance). High \\(H'\\) = many species, relatively even abundances, while low \\(H'\\) = few species or dominance by one/few species.\nTo calculate \\(H'\\) in R, here is the following code. Note, y\n\nshannon_output &lt;- vegan::diversity(site_counts[, -1], index = \"shannon\") # remove site column [,-1] for the diversity function to work\n\n# add H results to sites\nshannon_sum &lt;- bind_cols(site = site_counts$site,\n                               shannon = shannon_output)\n\n# Visualise results\nshannon_sum %&gt;%\n  ggplot() +\n  geom_bar(aes(x = site, y = shannon), stat = \"identity\") +\n  labs(x = \"Sites\", y = \"Shannon diveristy index (H')\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nIf \\(H'\\) = 0, only one species is present (no diversity). A value of 1.36 in site B means site B has slighty higher diversity than site A and site C. i.e.¬†more species and/or a more even distribution of individuals among species.\n\n\nEquation:\n\\[\nD_1 = 1 - \\sum_{i=1}^{S} p_i^2\n\\]\nwhere \\(D_1'\\) is Simpson‚Äôs diversity index, \\(S\\) is the total number of species in the community (species richness), \\(p_i\\) is the relative abundance of species \\(i\\) (see relative abundance). High \\(D_1\\) = high diversity, while low \\(D_1\\) = low diversity.\nFor inverse Simpson‚Äôs diversity index, the equation is:\n\\[\nD_2 = \\frac{1}{\\sum_{i=1}^{S} p_i^2}\n\\] where \\(D_2\\) is the inverse of Simpson‚Äôs diversity index. if \\(D_2\\) = 1, only one species is present, and if \\(D_2\\) is higher then more species are present/or more even distribution.\nTo calculate \\(D_1\\) in R, here is the following code.\n\nsimpson_output &lt;- vegan::diversity(site_counts[ , -1], index = \"simpson\") # remove site column [,-1] for the diversity function to work\n\n# add D1 results to sites\nsimpson_sum &lt;- bind_cols(site = site_counts$site,\n                               simpson = simpson_output)\n\n# Visualise results\nsimpson_sum %&gt;%\n  ggplot() +\n  geom_bar(aes(x = site, y = simpson), stat = \"identity\") +\n  labs(x = \"Sites\", y = \"Simpson's diveristy index (D1)\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nSimilar to Shannon diversity index, site B has higher \\(D_1\\) of 0.71 compared to the other two sites.\n\n\n\n\nExercise 3 (10 min)\nüß™ Complete the following task for the fake_data_clean below\n\nCalculate the Shannon diversity index, and Simpson‚Äôs diversity index by sites\nWhat is the diversity of site A using the Shannon diversity index (\\(H'\\)) method?\nWhat is the diversity of site B using the inverse Simpson‚Äôs diversity index (\\(D_2\\)) method?\n\n\n\nShow answer\n\n\n# sum species counts per site\nexcerise_survey_sum &lt;- excerise_survey_data %&gt;%\n  group_by(site) %&gt;%\n  summarise(across(where(is.numeric), sum))\n\n# Shannon diversity\nbind_cols(site = excerise_survey_sum$site, \n          shannon = vegan::diversity(excerise_survey_sum[, -c(1, 2)], index = \"shannon\"),\n          simpson = vegan::diversity(excerise_survey_sum[, -c(1, 2)], index = \"invsimpson\"))\n\n# 1. 1.14\n# 2. 3.5",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Diversity Indices</span>"
    ]
  },
  {
    "objectID": "week3_diversity_index.html#exercise-3-10-min",
    "href": "week3_diversity_index.html#exercise-3-10-min",
    "title": "3¬† Diversity Indices",
    "section": "Exercise 3 (10 min)",
    "text": "Exercise 3 (10 min)\nüß™ Use the previous R code and functions from the previous week to complete the following task for the fake_data_clean below",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Diversity Indices</span>"
    ]
  },
  {
    "objectID": "week2_tidy_data.html#exercise-15-min-1",
    "href": "week2_tidy_data.html#exercise-15-min-1",
    "title": "2¬† Tidy Data",
    "section": "Exercise (15 min)",
    "text": "Exercise (15 min)\nüß™ Clean the code below\nYour colleague got a new job and you have been promoted to his position where you are taking over his unfinished projects. The ecological data has been collected and some quick analysis was performed. But, you noticed that he didnt perform a quality check on the raw data and the # explanations are not clear. The current state is not suitable for the client so you are assigned to clean.\nClean the code where relevent, and check if all the code is working on your end.\nNote: change the working directory to your folder and load the plant_calcium.csv to work on. Note 2: The same errors to fix are from the week 2 - data quality workshop.\n\n# Exercise 2\n# Cleaning code and tidying dataset\n# John Smith, 25-03/2025\n\n## Set up ## ----------------------------------------------------------------\nlibrary(tiderVerse)\n\n# Set my directory\nsetwd(\"your working directory\")\n\n## Loading and cleaning data ## ---------------------------------------------\n\n# Load raw data\nplant data &lt;- read_csv(\"data/plant_calcium.csv\")\nstr(plant data)\n\n## Analysis ## -------------------------------------------------------------\n\n# Fit model: height explained by elevation\nmodel &lt;- lm(stem_dry_mass ~ stem_length, data = plant_data)\n\n# Check model summary\nsummary(model)\n\n## Visualisation ## --------------------------------------------------------\n\n# Creating scatterplot figure\nplant_data %&gt;%\n  ggplot() +\n  geom_point(aes(x = stem_length, y = Stem_dry_mass)) +\n  labs(\n    x = \"Stem Length (mm)\",\n    y = \"Stem Dry Mass (g)\",\n    title = \"Stem Dry Mass vs. Stem Length in Sugar Maple Seedlings\",\n    subtitle = \"Hubbard Brook LTER\"\n  ) +\n  theme_minimal()\n\n\n\nShow answer\n\n\n# Exercise 2\n# Cleaning code and tidying dataset\n# John Smith, 25-03-2025\n\n## Set up ## ----------------------------------------------------------------\nlibrary(tidyverse)\n\n# Set my directory\nsetwd(\"C:/Users/75002992/OneDrive - Murdoch University/Teaching/ECS200 - Research Methods in Ecology/ECS200 - Workshop/\")\n\n## Loading and cleaning data ## ---------------------------------------------\n\n# Load raw data\nplant_data &lt;- read_csv(\"data/plant_calcium.csv\")\nstr(plant_data)\n\n\n# Correct class type\nplant_data &lt;- plant_data %&gt;%\n  mutate(watershed = as.factor(watershed),\n         elevation = as.factor(elevation))\n\n# Check for missing values in each column\ncolSums(is.na(plant_data))\n\n# Visualise missing data pattern\nvisdat::vis_dat(plant_data_clean)\n\n# Remove NA rows based on missing elevation data\nplant_data &lt;- plant_data %&gt;% \n  drop_na(elevation)\n\n# Check for duplicate rows\nsum(duplicated(plant_data))  # None\n\n# Check for outliers\n\nggplot(plant_data, aes(x = leaf2area)) +\n  geom_histogram() +\n  theme_bw()\n\n# create a clean dataset after filtering the outlier\nplant_data_clean &lt;- plant_data %&gt;%\n  dplyr::filter(leaf2area &lt; 90)\n\n## Analysis ## -------------------------------------------------------------\n\n# Fit model: height explained by elevation\ndry_length_mod &lt;- lm(stem_dry_mass ~ stem_length, data = plant_data_clean)\n\n# Check model summary\nsummary(dry_length_mod)\n\n## Visualisation ## --------------------------------------------------------\n\n# Creating scatterplot figure\nplant_data_clean %&gt;%\n  ggplot() +\n  geom_point(aes(x = stem_length, y = stem_dry_mass)) +\n  labs(\n    x = \"Stem Length (mm)\",\n    y = \"Stem Dry Mass (g)\",\n    title = \"Stem Dry Mass vs. Stem Length in Sugar Maple Seedlings\",\n    subtitle = \"Hubbard Brook LTER\"\n  ) +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "week4_descriptive_stats.html#central-tendancy",
    "href": "week4_descriptive_stats.html#central-tendancy",
    "title": "4¬† Descriptive Stats",
    "section": "Central tendancy",
    "text": "Central tendancy\nDeciding on whether to use mean, median, or mode will depend on how your data is distributed. Here is an example of a skewed distribution and how calculations of central tendency can differ depending if you are using the mean, median, and mode.\n\nHere is a CrashCourse Statistics video on The Shape of Data: Distributions on other types of distributions encountered (11:22 min long). In practice, mode is rarely used, but is often included in a discussion of mean and medians. From here, we will focus on mean and median, which can be calculated using the mean() and median() function, respectively.\n\n#install.packages(\"cowplot\")\n\n# Load library\nlibrary(tidyverse)\nlibrary(cowplot)\n\n# Create fake data distribution\nset.seed(1)\ndist_data &lt;- data.frame(normal = rnorm(100, mean = 10, sd = 2),\n                        skewed = rexp(100, rate = 0.2))\n\ndist_data %&gt;%\n  dplyr::summarise(normal_mean = mean(normal),\n                   normal_median = median(normal),\n                   skewed_mean = mean(skewed),\n                   skewed_median = median(skewed))\n\n  normal_mean normal_median skewed_mean skewed_median\n1    10.21777      10.22782    4.656321      3.781882\n\n\nIf the data is normally distributed, the mean and median will be similar (e.g.¬†normal = 10), but if the data is skewed, the mean and median will show different central tenancies (e.g.¬†mean = 4.7, median = 3).",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Descriptive Stats</span>"
    ]
  },
  {
    "objectID": "week4_descriptive_stats.html#spread",
    "href": "week4_descriptive_stats.html#spread",
    "title": "4¬† Descriptive Stats",
    "section": "Spread",
    "text": "Spread\nWe can calculate the standard deviation, standard error, confidence interval, coefficient of variation, and variance with the following code can be calculated using the mean() and median() function, respectively.\n\ndist_data %&gt;%\n  dplyr::summarise(\n    n = length(skewed),\n    mean = mean(skewed), # required for calculation of CI\n    variance = var(skewed),\n    sd = sd(skewed), # standard deviation\n    se = sd(skewed) / sqrt(n), # standard error\n    ci_lower = mean(skewed) - qt(0.975, df = n - 1) * se, # lower 5%\n    ci_upper = mean(skewed) + qt(0.975, df = n - 1) * se, # lower 95%\n    cv = sd(skewed) / mean(skewed) # coefficient of variation\n  )\n\n    n     mean variance       sd        se ci_lower ci_upper       cv\n1 100 4.656321 13.85892 3.722757 0.3722757 3.917645 5.394997 0.799506\n\n\nRemember that the skewed data is strongly right-skewed so SD and CI must be interpreted carefully. SD measures spread around the mean assuming deviations are roughly symmetric. 95% CI assumes the sampling distribution of the mean is approximately normal. Skewed data violate this assumption. SE & CI describe uncertainty in the mean, not variability of the data.\n\ndist_dat_long &lt;- dist_data %&gt;%\n  tidyr::pivot_longer(cols = everything(), names_to = \"data_type\", values_to = \"value\")\n\ndist_dat_stats &lt;- dist_dat_long %&gt;%\n  group_by(data_type) %&gt;%\n  summarise(mean = mean(value),\n            sd = sd(value),\n            n = length(value),\n            se = sd(value) / sqrt(n),\n            ci_lower = mean(value) - qt(0.975, df = n - 1) * se, #lower 5%\n            ci_upper = mean(value) + qt(0.975, df = n - 1) * se, # lower 5%\n            cv = sd(value) / mean(value) # coefficient of variation\n            )\n\nsd_plot &lt;- ggplot(dist_dat_long, aes(x = value)) +\n  geom_histogram(bins = 30, fill = \"lightgrey\", colour = NA) +\n  geom_vline(data = dist_dat_stats, aes(xintercept = mean), colour = \"black\", linewidth = 1) +\n  geom_vline(data = dist_dat_stats, aes(xintercept = mean + sd), linetype = \"dashed\", colour = \"black\") +\n  geom_vline(data = dist_dat_stats, aes(xintercept = mean - sd), linetype = \"dashed\", colour = \"black\") +\n  facet_wrap(~ data_type, scales = \"free\") +\n  labs(\n    subtitle = \"Mean ¬± SD assumes symmetry\",\n    x = \"Value\",\n    y = \"Frequency\"\n  ) +\n  theme_classic()\n\nci_plot &lt;- ggplot(dist_dat_long, aes(x = value)) +\n  geom_histogram(bins = 30, fill = \"lightgrey\", colour = NA) +\n  geom_vline(data = dist_dat_stats, aes(xintercept = mean), colour = \"black\", linewidth = 1) +\n  geom_vline(data = dist_dat_stats, aes(xintercept = ci_lower), linetype = \"dashed\", colour = \"black\") +\n  geom_vline(data = dist_dat_stats, aes(xintercept = ci_upper), linetype = \"dashed\", colour = \"black\") +\n  facet_wrap(~ data_type, scales = \"free\") +\n  labs(\n    subtitle = \"Mean ¬± 95% CI assumes symmetry\",\n    x = \"Value\",\n    y = \"Frequency\"\n  ) +\n  theme_classic()\n\ncowplot::plot_grid(sd_plot, ci_plot, ncol = 1, align = \"v\")\n\n\n\n\n\n\n\n\nBoxplots are better alternatives to represent spread for skewed data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Descriptive Stats</span>"
    ]
  },
  {
    "objectID": "week4_descriptive_stats.html#other-descriptive-stats",
    "href": "week4_descriptive_stats.html#other-descriptive-stats",
    "title": "4¬† Descriptive Stats",
    "section": "Other descriptive stats",
    "text": "Other descriptive stats\nOther types of descriptive stats include quantiles, maximum, minimum, and range. Quantiles divide a sorted dataset or probability distribution into equal-sized subgroups. The max, min, and range can be calculated within the summarise() function, while the quantiles can be calculated with the base R quantile() function.\n\ndist_data %&gt;%\n  dplyr::summarise(normal_max = max(normal),\n                   normal_min = min(normal),\n                   normal_range = normal_max - normal_min)\n\n  normal_max normal_min normal_range\n1   14.80324     5.5706     9.232635\n\nquantile(dist_data$normal)\n\n       0%       25%       50%       75%      100% \n 5.570600  9.011515 10.227818 11.383091 14.803236 \n\n\nVisually for normal and skewed distributions, the quantiles would look like this. We will need to convert the wide format of dist_data to long format.\n\ndist_data %&gt;%\n    tidyr::pivot_longer(everything(), names_to = \"data_type\", values_to = \"value\") %&gt;% # convert wide format to long format\nggplot(aes(x = data_type, y = value)) +\n  geom_boxplot() +\n  theme_classic()",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Descriptive Stats</span>"
    ]
  },
  {
    "objectID": "week4_descriptive_stats.html#plot-types",
    "href": "week4_descriptive_stats.html#plot-types",
    "title": "4¬† Descriptive Stats",
    "section": "Plot Types",
    "text": "Plot Types\nThere are so many ways to visualise your data beyond this workshop, so we will only focus a few common ones you will likely use in your report.\n\nLet‚Äôs say we are interested in visualising at differences in stem length in calcium-treated (W1) versus untreated (Reference) watersheds, there are a couple of ways we can visualise this.\n\n# Load a CSV file\nplant_task_clean_data &lt;- read_csv(\"C:/Users/75002992/OneDrive - Murdoch University/Teaching/ECS200 - Research Methods in Ecology/ECS200 - Workshop/data/plant_calcium_task.csv\") %&gt;%\n  dplyr::mutate(watershed = recode(watershed, \"W2\" = \"W1\"),\n                watershed = as.factor(watershed),\n                transect  = as.factor(transect)) %&gt;%\n  dplyr::filter(stem_dry_mass &lt; 1)\n\n\nHistogram or densityBoxplotBar plot or dot plotScatter plot\n\n\nA histogram and density plot takes as input a numeric variable only. The variable is cut into several bins, and the number of observation per bin is represented by the height of the bar. It is possible to represent the distribution of several variable on the same axis using this technique.\nTo create a histogram by group, use the geom_histogram() function within ggplot.\n\nplant_task_clean_data %&gt;%\n  ggplot(aes(x = stem_length, fill = watershed)) +\n  geom_histogram(alpha = 0.4, position = \"identity\") # changed transparency due to overlap in watershed groups\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\nTo create a density plot by group, use the geom_density() function within ggplot.\n\nplant_task_clean_data %&gt;%\n  ggplot(aes(x = stem_length, fill = watershed)) +\n  geom_density(alpha = 0.4, position = \"identity\") # changed transparency due to overlap in watershed groups\n\n\n\n\n\n\n\n\n\n\nA boxplot provides an effective summary of one or more numeric variables, showcasing key statistical features through its distinct elements:\n\nMedian Line: The line that divides the box represents the median of the data. For example, if the median is 10, this indicates that half of the data points lie below 10 and half above.\nQuartiles: The ends of the box indicate the upper (Q3) and lower (Q1) quartiles. If Q3 is 15, this means that 75% of the observations fall below this value.\nInterquartile Range (IQR): The difference between Quartiles 1 and 3 is known as the interquartile range (IQR), which measures the spread of the middle 50% of the data.\nWhiskers: The lines extending from the box show the range of values within Q3 + 1.5 √ó IQR to Q1 - 1.5 √ó IQR, representing the highest and lowest values, excluding outliers.\nOutliers: Dots (or other markers) beyond the whiskers indicate potential outliers in the dataset.\n\n\nTo create a boxplot by group, use the geom_boxplot() function within ggplot.\n\nplant_task_clean_data %&gt;%\n  ggplot(aes(x = watershed, y = stem_length)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\nTo plot the mean and standard deviation as a bar plot, use the geom_bar() function with geom_errorbar().\n\nplant_sum &lt;- plant_task_clean_data %&gt;%\n  group_by(watershed) %&gt;%\n  summarise(mean = mean(stem_length),\n            sd = sd(stem_length))\n\n\nplant_sum %&gt;%\n  ggplot(aes(x = watershed)) +\n  geom_bar(aes(y = mean), stat = \"identity\") +\n  geom_errorbar(aes(ymin = mean-sd, ymax = mean+sd), width = .02)\n\n\n\n\n\n\n\n\nTo plot the mean and standard deviation as a dot plot, use the geom_pointrange(). It is also useful to include the raw data in this case, which you will need to call in the plant_task_clean_data and plant_sum separately and plot the raw data with the geom_jitter() function.\n\n# mean +/- sd only.\nmean_plot &lt;- plant_sum %&gt;%\n  ggplot() + \n  geom_pointrange(aes(x = watershed, y = mean, ymin = mean-sd, ymax = mean+sd))\n\n# including raw data\nraw_plot &lt;- ggplot() + \n  geom_jitter(data = plant_task_clean_data, aes(x = watershed, y = stem_length), \n              alpha = 0.1, position = position_jitter(0.2)) + #raw data\n  geom_pointrange(data = plant_sum, aes(x = watershed, y = mean, \n                                      ymin = mean-sd, ymax = mean+sd))\n\ncowplot::plot_grid(mean_plot, raw_plot, ncol = 2, align = \"v\")\n\n\n\n\n\n\n\n\n\n\nA scatterplot displays the relationship between 2 numeric variables. For each data point, the value of its first variable is represented on the X axis, the second on the Y axis\nTo create a scatterplot, use the geom_point() function within ggplot.\n\nplant_task_clean_data %&gt;%\n  ggplot(aes(x = stem_length, y = stem_dry_mass)) +\n  geom_point()\n\n\n\n\n\n\n\n\nTo create a scatterplot by group, include a colour or shape level within ggplot.\n\ncolor_plot &lt;- plant_task_clean_data %&gt;%\n  ggplot(aes(x = stem_length, y = stem_dry_mass, colour = watershed)) +\n  geom_point()\n\nshape_plot &lt;- plant_task_clean_data %&gt;%\n  ggplot(aes(x = stem_length, y = stem_dry_mass, shape = watershed)) +\n  geom_point()\n\ncowplot::plot_grid(color_plot, shape_plot, ncol = 1, align = \"v\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Descriptive Stats</span>"
    ]
  },
  {
    "objectID": "week4_descriptive_stats.html#tweaking-plots",
    "href": "week4_descriptive_stats.html#tweaking-plots",
    "title": "4¬† Descriptive Stats",
    "section": "Tweaking plots",
    "text": "Tweaking plots\nTo make your figures publication ready, here are some advice on changing the labels, adding colour, changing the shape and size, plotting multiple plots.\n\nThemesLabels and titlesShapes, colours, and sizeSplitting plots\n\n\nggplot has some default themes you can use for your figures instead of manually changing each feature. Here is what the themes look like when you apply them.\n\n\n\nTo add a labels independently for the title using ggtitle(), subtitle using ggsubtitle(), x-axis label using xlab(), y-axis label using ylab(), or all within the labs() function as shown here:\n\nplant_task_clean_data %&gt;%\n  ggplot(aes(x = watershed, y = stem_length)) +\n  geom_boxplot() +\n  # Modify here\n  labs(title = \"Your title\",\n       subtitle = \"your subtile\",\n       x = \"Treatment\", \n       #x = NULL, # You can also remove labels by calling NULL\n       y = \"Stem length (mm)\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nMore options can be found here such as changing the size and colour of the text, or spliting long titles into multiple lines.\n\n\nTo manually change the point shapes, use the scale_shape_manual() function, and choose the shape with it‚Äôs associated number. More options such as shape size can be found here.\n\n\nplant_task_clean_data %&gt;%\n  ggplot(aes(x = stem_length, y = stem_dry_mass, shape = watershed)) +\n  geom_point() +\n  # Modify here\n  scale_shape_manual(values = c(4, 15))+\n  labs(x = \"Stem length (mm)\", \n       y = \"Stem dry mass (g)\",\n       shape = \"Watershed\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nThere is a vast number of R libraries that that are tailored to create pretty palettes. You can explore them in your own time. Here is an R package called colorspace which I like to use often.\nYou can change the outline or fill colour overall or by groups (e.g.¬†watershed) with either method below. Note, you can either use existing R base colour like tomato1, specific R package color names, or hex values (e.g.¬†#000000 for black) for very specific colours.\n\nsingle_colour &lt;- plant_task_clean_data %&gt;%\n  ggplot(aes(x = watershed, y = stem_length)) +\n  geom_boxplot(colour = \"darkred\") + # Modify here\n  labs(subtitle = \"Change a single outline colour\",\n       x = \"Treatment\", \n       y = \"Stem length (mm)\") +\n  theme_classic()\n\nsingle_fill &lt;- plant_task_clean_data %&gt;%\n  ggplot(aes(x = watershed, y = stem_length)) +\n  geom_boxplot(fill = \"#F5C4B8\") + # Modify here\n  labs(subtitle = \"Change a single fill colour\",\n       x = \"Treatment\", \n       y = \"Stem length (mm)\") +\n  theme_classic()\n\ngroup_colour &lt;- plant_task_clean_data %&gt;%\n  ggplot(aes(x = watershed, y = stem_length, colour = watershed)) + # Modify here\n  geom_boxplot() +\n  # Modify here\n  scale_color_manual(values = c(\"darkblue\", \"darkred\")) +\n  labs(subtitle = \"Change outline colour by group\",\n       x = \"Treatment\", \n       y = \"Stem length (mm)\",\n       colour = \"Watershed\") +\n  theme_classic()\n\ngroup_fill &lt;- plant_task_clean_data %&gt;%\n  ggplot(aes(x = watershed, y = stem_length, fill = watershed)) + # Modify here\n  geom_boxplot() +\n  # Modify here\n  scale_fill_manual(values = c(\"#B8CEF5\", \"#F5C4B8\")) +\n  labs(subtitle = \"Change fill colour by group\",\n       x = \"Treatment\", \n       y = \"Stem length (mm)\",\n       fill = \"Watershed\") +\n  theme_classic()\n\ncowplot::plot_grid(single_colour, single_fill, \n                   group_colour, group_fill, ncol = 2, align = \"v\")\n\n\n\n\n\n\n\n\nYou can also colour by gradient if the colour values are continuous. Here is an example with a scatter plot.\n\ncat_plot &lt;- plant_task_clean_data %&gt;%\n  ggplot(aes(x = stem_length, y = stem_dry_mass, colour = watershed)) +\n  geom_point() +\n  # Modify here\n  scale_colour_manual(values = c(\"#B8CEF5\", \"#F5C4B8\")) +\n  labs(subtitle = \"Fill by categorical group\",\n       x = \"Stem length (mm)\", \n       y = \"Stem dry mass (g)\",\n       colour = \"Watershed\") +\n  theme_classic()\n\n\ncon_plot &lt;- plant_task_clean_data %&gt;%\n  ggplot(aes(x = stem_length, y = stem_dry_mass, colour = leaf_dry_mass)) +\n  geom_point() +\n    labs(subtitle = \"Fill by continuous value\",\n       x = \"Stem length (mm)\", \n       y = \"Stem dry mass (g)\",\n       colour = \"Leaf dry mass (g)\") +\n  theme_classic()\n\ncowplot::plot_grid(cat_plot, con_plot, ncol = 1, align = \"v\")\n\n\n\n\n\n\n\n\n\n\nThe facet approach partitions a plot into a matrix of panels. Each panel shows a different subset of the data. Let‚Äôs seperate out plant_task_clean_data by year using the facet_grid() function.\n\n# Split in vertical direction\nplant_task_clean_data %&gt;%\n  ggplot(aes(x = stem_length, y = stem_dry_mass, colour = watershed)) +\n  geom_point() +\n  scale_colour_manual(values = c(\"#B8CEF5\", \"#F5C4B8\")) +\n  labs(x = \"Stem length (mm)\", \n       y = \"Stem dry mass (g)\",\n       colour = \"Watershed\") + \n  theme_classic() +\n  # Modify here\n  facet_grid(year ~ .)\n\n\n\n\n\n\n\n# Split in horizontal direction\nplant_task_clean_data %&gt;%\n  ggplot(aes(x = stem_length, y = stem_dry_mass, colour = watershed)) +\n  geom_point() +\n  scale_colour_manual(values = c(\"#B8CEF5\", \"#F5C4B8\")) +\n  labs(x = \"Stem length (mm)\", \n       y = \"Stem dry mass (g)\",\n       colour = \"Watershed\") + \n  theme_classic() +\n  # Modify here\n  facet_grid(. ~ year)\n\n\n\n\n\n\n\n\nFacets can be placed side by side using the function facet_wrap() as followed. Here, we switch to have the years coloured and the plot split by watershed. How you decide which variable to split and colour will depend on your story and hypothesis.\n\nplant_task_clean_data %&gt;%\n  ggplot(aes(x = stem_length, y = stem_dry_mass, colour = factor(year))) +\n  geom_point() +\n  scale_colour_manual(values = c(\"#B8CEF5\", \"#F5C4B8\")) +\n  labs(x = \"Stem length (mm)\", \n       y = \"Stem dry mass (g)\",\n       colour = \"Watershed\") + \n  theme_classic() +\n  # Modify here\n  facet_wrap(~ watershed)\n\n\n\n\n\n\n\n\nIf you have different types of plots you would like to join together, the R package cowplot does a good job of this. More details on cowplot can be found here, but here is a taste of how to use cowplot. The figure legends are position at the bottom to look nicer using theme(legend.position=\"bottom\").\n\ncowplot::plot_grid(group_fill + theme(legend.position = \"bottom\"), \n                   cat_plot + theme(legend.position = \"bottom\"), \n                   ncol = 2, align = \"v\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Descriptive Stats</span>"
    ]
  },
  {
    "objectID": "week4_descriptive_stats.html#time-series",
    "href": "week4_descriptive_stats.html#time-series",
    "title": "4¬† Descriptive Stats",
    "section": "Time series",
    "text": "Time series\nMany of you might be working with environmental data (e.g.¬†soil or air temperatures and humidity) collected from deployed data loggers. While, you may not not use the whole time series dataset for your study, it could useful to visualise the study period and highlight where you took your samples (e.g.¬†using geom_vline().\nHere is a dataset of outside (EXT) and inside (DEEP) cave air temperature and humidity of two sites (MAM, YES). Using everything we learnt today, you can create a figure like this using the geom_line() function and separating temperature and humidity by location (EXT, DEEP) and site (MAM, YES).\nThe code below also includes ways of converting raw date values into meaningful values that R can interpret correctly (with as.POSIXct() function). The link also discusses ways of dealing with missing and weird time series data.\n\nclim_raw &lt;- read_csv(\"C:/Users/75002992/OneDrive - Murdoch University/Teaching/ECS200 - Research Methods in Ecology/ECS200 - Workshop/data/winter_clim_dat.csv\") %&gt;%\n  dplyr::mutate(date_time = as.POSIXct(date_time, format = \"%d/%m/%Y %H:%M\"), # convert character to date and time\n                DOY       = as.POSIXlt(date_time)$yday, # calculate day of the year\n                month     = format(as.Date(date_time, format = \"%m/%d/%Y\"),\"%m\"), # month as numeric\n                month_cat = format(as.Date(date_time, format = \"%m/%d/%Y\"),\"%B\"), # month as name\n                location =  factor(location, levels = c('EXT', 'DEEP')))  \n\ntemp_plot &lt;- clim_raw %&gt;% \n  ggplot() +\n  geom_line(aes(x = date_time, y = air_temp, colour = location)) +\n  scale_colour_manual(values = c(\"#F5C4B8\", \"darkred\")) + # manually change colour\n  geom_vline(xintercept = as.POSIXct(as.Date(\"2023-07-10\")), # add horizontal line when data were sampled\n                                     linetype = \"dashed\") + \n  geom_vline(xintercept = as.POSIXct(as.Date(\"2023-08-12\")),\n                                     linetype = \"dashed\") + \n  labs(x = \"Date\", \n       y = \"Air temperature (¬∞C)\") + \n  theme_classic() +\n  facet_wrap(~ site) # separate by site\n\n\nRH_plot &lt;- clim_raw %&gt;% \n  ggplot() +\n  geom_line(aes(x = date_time, y = RH, colour = location)) +\n  scale_colour_manual(values = c(\"#B8CEF5\", \"darkblue\")) + # manually change colour\n    geom_vline(xintercept = as.POSIXct(as.Date(\"2023-07-10\")), # add horizontal line when data were sampled\n                                     linetype = \"dashed\") + \n  geom_vline(xintercept = as.POSIXct(as.Date(\"2023-08-12\")),\n                                     linetype = \"dashed\") + \n  labs(x = \"Date\", \n       y = \"Relative humidity (%)\") + \n  theme_classic() +\n  facet_wrap(~ site) # separate by site\n  \ncowplot::plot_grid(temp_plot, \n                   RH_plot, \n                   ncol = 1, align = \"v\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Descriptive Stats</span>"
    ]
  },
  {
    "objectID": "week4_descriptive_stats.html#additional-resource",
    "href": "week4_descriptive_stats.html#additional-resource",
    "title": "4¬† Descriptive Stats",
    "section": "Additional resource",
    "text": "Additional resource\n\nMore detail on levels of measurements: norminal, ordinal, interval and ratio.\nggplot2 Essentials has all the guide for creating the most common types of graphs from bar and line plots, to pie charts.\nA collection of R chart examples with example code to create them.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Descriptive Stats</span>"
    ]
  },
  {
    "objectID": "week4_descriptive_stats.html#beautify-figures",
    "href": "week4_descriptive_stats.html#beautify-figures",
    "title": "4¬† Descriptive Stats",
    "section": "Beautify figures",
    "text": "Beautify figures\nIf you want to go beyond what we learnt and further enhance your figure production skills, here is a link that provides a tutorial to convert default ggplots to publication ready plots including this one on histograms .",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Descriptive Stats</span>"
    ]
  }
]